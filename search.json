[
  {
    "objectID": "CopulaIntro.html",
    "href": "CopulaIntro.html",
    "title": "Copulas",
    "section": "",
    "text": "A common goal is the modelling of dependence between outcomes, however this is only trivially/commonly implemented for Gaussian or Student-T distributed outcomes.\nAn alternative, more flexible approach is to model the dependence structure separately from the marginal distributions. This is the idea behind copula modelling.\n\n\nA copula function allows for the construction of a multivariate cumulative distribution function (CDF) from a set of univariate CDFs (for each marginal), with no requirement that the marginals are Gaussian or Student-T distributed - or that they all have the same distribution.\nGiven a set of outcomes \\(x_1, ..., x_m\\), with marginal CDFs \\(F_1, ..., F_m\\), and the results of applying these CDFs to the outcomes \\(u_1 = F_1(x_1), ..., u_m = F_m(x_m)\\), the copula function \\(C\\) is defined as:\n\\[\nC(u | \\Gamma) = G_m(G^{-1}(u_1), ..., G^{-1}(u_m)|\\Gamma)\n\\] Where \\(G_m\\) is an \\(m\\)-variate CDF for a given distribution, \\(G^{-1}\\) is the univariate quantile function (inverse-CDF) for that distribution, and \\(\\Gamma\\) is a correlation matrix.\nThe most popular implementation of this is the Gaussian copula:\n\\[\nC(u | \\Gamma) = \\Phi_m(\\Phi^{-1}(u_1), ..., \\Phi^{-1}(u_m)|\\Gamma)\n\\] Where \\(\\Phi_m\\) is the \\(m\\)-variate CDF for the standard normal distribution, and \\(\\Phi^{-1}\\) is the univariate quantile function for the standard normal distribution.\n\n\nNote that correlation matrix \\(\\Gamma\\) can no longer be interpreted on the scale of the observed outcomes. Instead, these correlations are now on the scale of the copula function. For example, when a Gaussian copula is used with continuous marginals the estimated correlation is linear between normal scores:\n\\[\n\\Gamma_{1,2} = \\text{Corr}(\\Phi^{-1}(F_1(x_1)), \\Phi^{-1}(F_2(x_2)))\n\\]\nWhen a Gaussian copula is used with a discrete marginal, the correlation is with a normally-distributed latent variate. For example, with two ordinal outcomes, the correlation would be interpreted as a polychoric correlation coefficient. This can also be used as a motivating reason for using the Gaussian copula, as opposed to other forms, as the estimated correlations have existing analogues in the literature with known interpretations.\n\n\n\n\nThe Copula approach to generating random variates also allows for the very simple generation of correlated/dependent data, regardless of their desired marginal distributions. To this, we essentially reverse the steps involved in a Gaussian copula - instead of transforming the observed outcomes to multivariate normal scores, we transform multivariate normal scores to the desired distributions.\n\\[\n\\begin{bmatrix}z_1\\\\ \\vdots \\\\ z_m \\end{bmatrix}\n\\sim \\textrm{MVN}\n\\left(\n  \\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\n  \\begin{bmatrix}\n    1 & \\dots & \\rho_{1m} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{m1} & \\dots & 1\n  \\end{bmatrix}\n\\right)\n\\]\n\\[\n\\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_m \\end{bmatrix} = \\begin{bmatrix}F_1^{-1}(\\Phi(z_1))\\\\ \\vdots \\\\ F_m^{-1}(\\Phi(z_m)) \\end{bmatrix}\n\\]\nWhere \\(F_i^{-1}(\\cdot)\\) is the quantile function for the \\(i\\)-th marginal distribution, and \\(\\Phi(\\cdot)\\) is the standard normal CDF.\nThis can be further simplified by using the Cholesky decomposition of the correlation matrix to construct the multivariate normal variates:\n\\[\nx \\sim \\textrm{N(0, 1)} \\Rightarrow z = Lx \\sim \\textrm{MVN}(0, LL^T)\n\\]\nWhere \\(L\\) is the Cholesky decomposition of the correlation matrix \\(\\Gamma\\).\nTo illustrate, consider the goal of generating an exponentially-distributed outcome \\(y_1\\) (with rate parameter \\(\\lambda\\)) and a chi-squared-distributed outcome \\(y_2\\) (with degrees of freedom \\(\\nu\\)) with a correlation of 0.5. This can be simply achieved by sampling from a univariate standard-normal, multiplying by the cholesky factor of the desired correlations, and then transforming these to the desired distributions:\n\nset.seed(2024)\nlambda &lt;- 2\nnu &lt;- 5\n\nL &lt;- chol(matrix(c(1, 0.5, 0.5, 1), nrow = 2))\nz &lt;- matrix(rnorm(1000), ncol = 2) %*% L\n\ny &lt;- pnorm(z)\ny[,1] &lt;- qexp(y[,1], rate = lambda)\ny[,2] &lt;- qchisq(y[,2], df = nu)\n\n\n\n\nWith the copula function defined, the joint density function with all continuous marginals is given as:\n\\[\nf(x) = c(u | \\Gamma) \\prod_{i=1}^{m} f_i(x_i)\n\\] Where \\(c(\\dot)\\) is the density function for the chosen copula function and \\(f_i\\) is the marginal density function for the \\(i\\)-th outcome.\n\n\nFor a Gaussian copula, the density function is given as:\n\\[\nc(u | \\Gamma) = |\\Gamma|^{-1/2} \\exp\\left(\\frac{q^T\\left(I_m - \\Gamma^{-1}\\right)q}{2}\\right)\n\\] \\[\n= |\\Gamma|^{-1/2} \\exp\\left(\\frac{q^Tq}{2} - \\frac{q^T\\Gamma^{-1}q}{2}\\right)\n\\]\nWhere \\(q = \\Phi^{-1}(u)\\).\nNote that when the outcomes are independent (i.e., \\(\\Gamma = I\\)), or there is only a single outcome, the joint density function simplifies to the product of the marginal density functions.\n\n\nThe form of the Gaussian copula density above is also now amenable to being expressed using a multivariate normal. Where the multivariate standard normal density is given as:\n\\[\n\\textrm{MVN}(q | 0, \\Gamma) = \\left(2\\pi\\right)^{-m/2} |\\Gamma|^{-1/2} \\exp\\left(-\\frac{q^T\\Gamma^{-1}q}{2}\\right)\n\\]\nThe Gaussian copula density can be expressed as a multivariate standard-normal density multipled by an adjustment term:\n\\[\nc(u | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\exp\\left(\\frac{q^Tq}{2}\\right) \\cdot \\left(2\\pi\\right)^{m/2}\n\\]\nThe adjustment term above is equal to the reciprocal of the standard-normal density evaluated at \\(q\\), giving the final form of the Gaussian copula density:\n\\[\nc(u | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\prod_{i=1}^m\\textrm{N}(q_i | 0, 1)^{-1}\n\\]\nThis form also makes intuitive sense. The Gaussian copula is specifying a multivariate normal density over a transformation of inputs (\\(q = \\Phi(u)\\)), so we need to adjust the density by the absolute derivative of the transformation to account for this. The derivative of the standard normal CDF \\(\\Phi()\\) is the reciprocal of the standard normal density, so we simply multiply the multivariate normal density by the product of these reciprocals.\nThis also provides other benefits, as we can simply delegate to existing implementations of these density functions which have already received significant optimisation and testing.\n\n\n\n\nTo illustrate, we can fit a bivariate Gaussian copula to the generated data from the previous example to see whether we can recover the data-generating parameters. We will first show to estimate these using simple maximum likelihood estimation in R, and then using a Bayesian approach.\nTo refresh, we have an exponentially-distributed outcome \\(y_1\\) with rate parameter \\(\\lambda\\), a chi-squared-distributed outcome \\(y_2\\) with degrees of freedom \\(\\nu\\), and we are modelling the dependence between these outcomes using a Gaussian copula with correlation \\(\\rho\\):\n\\[\ny_1 \\sim \\textrm{Exp}(\\lambda)\n\\]\n\\[\ny_2 \\sim \\chi^2(\\nu)\n\\]\n\\[\n\\begin{bmatrix}F_1(y_1) \\\\ F_2(y_2) \\end{bmatrix} \\sim \\textrm{c}_{\\Phi}\n\\left(\n  \\begin{bmatrix}\n    1 & \\rho \\\\\n    \\rho & 1\n  \\end{bmatrix}\n\\right)\n\\] Where \\(F_i(\\cdot)\\) is the CDF for the \\(i\\)-th marginal distribution, and \\(\\textrm{c}_{\\Phi}(\\cdot)\\) is the Gaussian copula density.\n\n\n\nTo estimate these parameters via maximum-likelihood in R, we need to define a function that takes a vector of parameter values and returns the joint log-likelihood. It is also useful to define the function such that the input parameters can take on any value, and then transform these to the appropriate scale within the function (i.e., \\(\\rho \\in \\left(-1,1\\right)\\)). Given that we will then be using transformations of the parameters to define the log-likelihood, we need to add a correction factor to account for this. For more information and examples of different transformations & corrections, see this Stan Documentation Page.\nFirst, we will define a function to calculate the gaussian copula density for a given set of uniform variates \\(u\\) and correlation matrix \\(\\Gamma\\):\n\ndgauss_copula &lt;- function(u, gamma, log = TRUE) {\n  # Map uniform variates to normal scores\n  q &lt;- qnorm(u)\n  ll &lt;- sum(RTMB::dmvnorm(q, Sigma = gamma, log = TRUE)) - sum(dnorm(q, log = TRUE))\n  ifelse(isTRUE(log), ll, exp(ll))\n}\n\n\n# Define the log-likelihood function\nll_function &lt;- function(parameters, data_y) {\n  # Constrain input parameters to valid values for distributions\n  lambda &lt;- exp(parameters[1])\n  nu &lt;- exp(parameters[2])\n  rho &lt;- tanh(parameters[3])\n\n  adjustments &lt;- sum(parameters[1:2]) + log1p(-(rho*rho))\n\n  marginal_log_lik &lt;- sum(dexp(data_y[,1], rate = lambda, log = TRUE)) +\n                      sum(dchisq(data_y[,2], df = nu, log = TRUE))\n\n  # Apply marginal CDFs to data\n  u &lt;- data_y\n  u[,1] &lt;- pexp(data_y[,1], rate = lambda)\n  u[,2] &lt;- pchisq(data_y[,2], df = nu)\n\n  copula_log_lik &lt;- dgauss_copula(u, matrix(c(1, rho, rho, 1), nrow = 2),\n                                  log = TRUE)\n\n  # Return joint log-likelihood with adjustments for change-of-variables\n  marginal_log_lik + copula_log_lik + adjustments\n}\n\nHowever, rather than returning the log-likelihood itself, we need to return the negative log-likelihood. This is because the optimisation functions in R are minimisation functions. To do this, we’ll simply create a ‘wrapper’ function which takes an arbitrary function as input and returns a function which itseld returns the negative of the input function’s output:\n\nneg_ll_function &lt;- function(f) {\n  function(...) { -f(...) }\n}\n\nNext we can use the optim function to estimate the parameters:\n\nresult &lt;- optim(\n  par = c(0, 0, 0), # initial values for parameters\n  fn = neg_ll_function(ll_function),\n  data_y = y\n)\n\nAnd extract and transform them to the appropriate scale, showing that the recovery was successful:\n\nc(\n  \"lambda\" = exp(result$par[1]),\n  \"nu\" = exp(result$par[2]),\n  \"rho\" = tanh(result$par[3])\n) |&gt; round(2)\n\nlambda     nu    rho \n  1.96   4.91   0.54 \n\n\n\n\n\nWe can also estimate these parameters using a Bayesian approach. This can be done in two ways: by using our existing ll_function() implementation for sampling, or by writing a Stan model to estimate the parameters.\n\n\nEstimating the posterior distributions for parameters of an R function can be done quite simply using the StanEstimators R package:\n\nif (!requireNamespace(\"StanEstimators\", quietly = TRUE)) {\n  remotes::install_github(\"andrjohns/StanEstimators\")\n}\nlibrary(StanEstimators)\nsuppressPackageStartupMessages(library(posterior))\n\nThis will allow us to use algorithms implemented by Stan to estimate parameters for arbitrary R functions. To use full Bayesian sampling with the No-U-Turn Sampler (NUTS) algorithm, we can use the stan_sample() function:\n\n# Record executtion for later comparison\nsampling_time &lt;- system.time({\n  results_stan &lt;- stan_sample(fn = ll_function,\n                              par_inits = c(0,0,0),\n                              grad_fun = \"RTMB\",\n                              additional_args = list(data_y = y),\n                              parallel_chains = 4,\n                              quiet = TRUE)\n})\n\nError in (function (parameters, data_y) : unused argument (v = new(\"advector\", , c(0+4.6421248212267e-310i, 4.94065645841247e-324+4.6421248212267e-310i, 9.88131291682493e-324+4.6421248212267e-310i)))\n\n\nNext, we can use the posterior package to extract the posterior draws for the parameters and transform them to the appropriate scale before summarising:\n\nresults_stan@draws |&gt;\n    mutate_variables(lambda = exp(`pars[1]`),  nu = exp(`pars[2]`),  rho = tanh(`pars[3]`)) |&gt;\n    subset_draws(variable=c(\"lambda\", \"nu\", \"rho\")) |&gt;\n    summarise_draws()\n\nError: object 'results_stan' not found\n\n\nWe can see that estimates are consistent with their maximum likelihood counterparts, and the narrow credible intervals and high effective sample sizes indicate that we can have confidence in the estimated values.\nHowever, full Bayesian sampling from an R function can be computationally expensive and inefficient. This is especially true in this case where we have not also provided a provided a function to calculate the gradients for each parameter. Without this, StanEstimators has to fall back to using finite differences to estimate the gradients, which can be slow and inaccurate.\n\n\n\nOne alternative, before moving to using a Stan model directly, is to use the Pathfinder algorithm for approximate Bayesian inference. As the name implies, this allows us to approximate the results from full Bayesian sampling with dramatically reduced computational cost:\n\npath_time &lt;- system.time({\n  results_path &lt;- stan_pathfinder(fn = ll_function,\n                                  par_inits = c(0,0,0),\n                                  grad_fun = \"RTMB\",\n                                  additional_args = list(data_y = y),\n                                  quiet = TRUE,\n                            # Return same number of approximate draws as sampling\n                            num_psis_draws = 4000)\n})\n\nError in (function (parameters, data_y) : unused argument (v = new(\"advector\", , c(0+4.64212144286826e-310i, 4.94065645841247e-324+4.64212144286826e-310i, 9.88131291682493e-324+4.64212144286826e-310i)))\n\n\nWe can see that this provided posterior estimates consistent with sampling:\n\nresults_path@draws |&gt;\n    mutate_variables(lambda = exp(`pars[1]`),  nu = exp(`pars[2]`),  rho = tanh(`pars[3]`)) |&gt;\n    subset_draws(variable=c(\"lambda\", \"nu\", \"rho\")) |&gt;\n    summarise_draws()\n\nError: object 'results_path' not found\n\n\nWhile the execution time was orders of magnitude faster:\n\nc(\"Stan\" = sampling_time[3], \"Pathfinder\" = path_time[3])\n\nError: object 'sampling_time' not found\n\n\nFor more efficient Bayesian sampling, especially with larger or more complex models, it is recommended to use a Stan model directly.\n\n\n\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0&gt; nu;\n  cholesky_factor_corr[2] rho_chol;\n}\n\nmodel {\n  matrix[N, 2] u;\n\n  for (n in 1:N) {\n    u[n, 1] = exponential_cdf(Y[n, 1] | lambda);\n    u[n, 2] = chi_square_cdf(Y[n, 2] | nu);\n  }\n\n  Y[, 1] ~ exponential(lambda);\n  Y[, 2] ~ chi_square(nu);\n\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n}\n\nWe will be using the cmdstanr package for our model fitting:\n\nif (!requireNamespace(\"cmdstanr\", quietly = TRUE)) {\n  remotes::install_github(\"stan-dev/cmdstanr\")\n  cmdstanr::check_cmdstan_toolchain(fix = TRUE)\n}\n\nFirst, we create a model object from the above Stan code, which has been saved to a file named gauss_copula_continuous.stan:\n\ncopula_mod &lt;- cmdstan_model(\"gauss_copula_continuous.stan\")\n\nThen we can use the compiled model object to sample our parameters:\n\ncopula_fit &lt;- copula_mod$sample(data = list(N = nrow(y), Y = y),\n                                parallel_chains = 4)\n\nWe can see that the results are consistent with the estimates from our StanEstimators implementation:\n\ncopula_fit$summary(variables = c(\"lambda\", \"nu\", \"rho\"))\n\n# A tibble: 3 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda   1.96   1.96  0.0829 0.0842 1.83  2.10   1.00    2741.    3058.\n2 nu       4.91   4.91  0.120  0.118  4.72  5.11   1.00    2999.    2482.\n3 rho      0.536  0.538 0.0280 0.0274 0.487 0.580  1.00    2742.    2583.\n\n\nBut the time taken to fit the model is significantly faster:\n\ncopula_fit$time()$total\n\n[1] 10.76354\n\n\n\n\n\n\n\n\n\nWhile the use of copulas are attractive for continuous marginals, their application to discrete marginals is less straightforward. It was possible to analytically define the density function for continuous marginals as there was a unique, one-to-one mapping from the observed, marginal scale (\\(x_m\\)) to copula function scale (\\(q = \\Phi^{-1}(u)\\)). However, this is not the case for discrete marginals, as there are now a range of \\(u_m\\) values that map to the same \\(x_m\\) value.\nFor discrete marginals, we are instead estimating a probability mass function (PMF): summing the copula density function over all possible values of \\(u_m\\) that map to the observed \\(x_m\\) value.\nNote that the PMF for a given univariate discrete distribution can be expressed as:\n\\[\nf(x) = F(x) - F(x^-)\n\\] Where \\(x^-\\) denotes the previous value of \\(x\\) (or 0 if \\(x == 0\\)).\nThis is extended to the multivariate case by summing, for each \\(x_m\\), the difference between the copula function evaluated at \\(x_m\\) and \\(x_m^-\\), holding all other \\(x_i\\) constant. In other words, we need to integrate the uniform \\(u_m\\) variable out of the likelihood. This is expressed as:\n\\[\nf(x) = \\sum_{j_1=1}^2 \\cdot\\cdot\\cdot \\sum_{j_m=1}^2 (-1)^{j_1 + \\cdot\\cdot\\cdot + j_m} C(u_{1,j_1}, \\cdot\\cdot\\cdot, u_{m,j_m} | \\Gamma)\n\\] Where \\(u_{j,1}\\) = \\(F_j(x_j^-)\\) and \\(u_{j,2}\\) = \\(F_j(x_j)\\).\nNote that this process is markedly more computationally costly than the continuous case, particularly as the number of outcomes increases - as an additional dimension of integration is added for each outcome. Additionally, evaluating the density for copula with discrete marginals now requires the evaluation of the \\(m\\)-variate CDF, where the continuous case only required the evaluation of the \\(m\\)-variate density. While there are existing implementations for the multivariate Gaussian and Student-T CDFs, these require numerical integration and so are markedly more costly than the corresponding density functions.\nUsing the bivariate case as an example:\n\\[\nf(x_1, x_2) = \\sum_{j_1=1}^2 \\sum_{j_2=1}^2 (-1)^{j_1 + j_2} C(u_{1,j_1}, u_{2,j_2} | \\Gamma)\n\\]\n\\[\n= (-1)^{1+1} C(u_{1,1}, u_{2,1} | \\Gamma)\n\\]\n\\[\n+ (-1)^{1+2} C(u_{1,1}, u_{2,2} | \\Gamma)\n\\]\n\\[\n+ (-1)^{2+1} C(u_{1,2}, u_{2,1} | \\Gamma)\n\\]\n\\[\n+ (-1)^{2+2} C(u_{1,2}, u_{2,2} | \\Gamma)\n\\]\nWhich is more simply expressed as:\n\\[\nf(x_1, x_2) =\n\\] \\[\nC(F_1(x_1^-), F_2(x_2^-) | \\Gamma) \\\\\n\\]\n\\[\n  - C(F_1(x_1), F_2(x_2^-) | \\Gamma) \\\\\n\\]\n\\[\n  - C(F_1(x_1^-), F_2(x_2) | \\Gamma) \\\\\n\\]\n\\[\n  + C(F_1(x_1), F_2(x_2) | \\Gamma)\n\\]\nWhere \\(C(\\dot)\\) is the copula function. Note that in the univariate case, this simplifies to the marginal PMF:\n\\[\nf(x_1 | I_1) = \\Phi_1(\\Phi^{-1}(F_1(x_1)) | I_1) - \\Phi_1(\\Phi^{-1}(F_1(x_1^-)) | I_1) \\\\\n\\] \\[\n= \\Phi(\\Phi^{-1}(F_1(x_1))) - \\Phi(\\Phi^{-1}(F_1(x_1^-))) \\\\\n\\] \\[\n= F_1(x_1) - F_1(x_1^-)\n\\]\nAs \\(\\Phi_1(\\cdot | I_1) == \\Phi(\\cdot)\\) (i.e., a multivariate CDF with one outcome and unit variance is equivalent to the univariate CDF). \n\n\n\nClearly, directing estimating the likelihood by marginalising over the uniform variables is computationally expensive. A more efficient alternative was proposed by Smith & Khaled (2011), which was the use of data augmentation to treat the \\(u_m\\) uniform variables as parameters to be estimated. In other words, for each observation we estimate a uniform parameter bounded by the marginal CDF evaluated at the upper and lower bounds:\n\\[\nu_m \\sim U\\left(F_m(x_m^-), F_m(x_m)\\right)\n\\]\nIn Smith & Khaled’s (2011) original paper, this still required some computational complexity to implement, as they were using a Gibbs sampler - requiring the specification of conditional distributions. This is much simpler in Stan (and other HMC samplers), where we only need to specify the joint density function. This means that we simply specify the uniform parameters and use them in the copula density function that we defined above.\nTo illustrate, we will use a Gaussian copula to model the correlation \\(\\rho\\) between a Poisson-distributed outcome \\(y_1\\) with rate parameter \\(\\lambda\\) and a binomial outcome \\(y_2\\) with probability \\(\\theta\\).\nAs with the example above, we will first use the copula approach to generate the correlated outcomes:\n\nlambda_r &lt;- 10\ntheta_r &lt;- 0.7\nrho_r &lt;- 0.5\ny_denom &lt;- sample(1:100, 500, replace = TRUE)\n\nz_discrete &lt;- matrix(rnorm(1000), ncol = 2) %*%\n                chol(matrix(c(1, rho_r, rho_r, 1), nrow = 2))\ny_discrete &lt;- pnorm(z_discrete)\ny_discrete[,1] &lt;- qpois(y_discrete[,1], lambda_r)\ny_discrete[,2] &lt;- qbinom(y_discrete[,2], size = y_denom, prob = theta_r)\n\nNext, we update our Stan model with the data augmentation approach. As Stan allows us to specify parameter bounds directly, we simply need a function which evaluates the marginal CDFs at the upper and lower bounds for each observation and returns these as the bounds for the uniform parameters. This is implemented as the uvar_bounds function in the Stan model below.\nThen the u parameters are used directly in the copula density function:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(array[] int pois_y, array[] int binom_y,\n                     array[] int binom_N, real lambda, real theta,\n                     int is_upper) {\n    int N = size(pois_y);\n    matrix[N, 2] u_bounds;\n\n    for (n in 1:N) {\n      if (is_upper == 0) {\n        u_bounds[n, 1] = pois_y[n] == 0.0\n                          ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n        u_bounds[n, 2] = binom_y[n] == 0.0\n                          ? 0.0 : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[n, 1] = poisson_cdf(pois_y[n] | lambda);\n        u_bounds[n, 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int pois_y;\n  array[N] int binom_y;\n  array[N] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(pois_y, binom_y, binom_N, lambda, theta, 0),\n    upper=uvar_bounds(pois_y, binom_y, binom_N, lambda, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[2] rho_chol;\n}\n\nmodel {\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n}\n\nNext we follow the same process as the continuous case to fit the model:\n\ncopula_discrete_mod &lt;- cmdstan_model(\"gauss_copula_discrete.stan\")\n\nThen we can use the compiled model object to sample our parameters:\n\ncopula_discrete_fit &lt;- copula_discrete_mod$sample(\n                                data = list(N = nrow(y_discrete),\n                                            pois_y = y_discrete[,1],\n                                            binom_y = y_discrete[,2],\n                                            binom_N = y_denom),\n                                parallel_chains = 4)\n\nWe can see that we were able to successfully recover both the marginal and correlation data-generating parameters:\n\ncopula_discrete_fit$summary(variables = c(\"lambda\", \"theta\", \"rho\"))\n\n# A tibble: 3 × 10\n  variable   mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda   10.2   10.2   0.139   0.139   9.97  10.4    1.00    4351.    3464.\n2 theta     0.699  0.699 0.00292 0.00280 0.694  0.703  1.00    4831.    2970.\n3 rho       0.519  0.520 0.0304  0.0305  0.468  0.566  1.00    4375.    2938.\n\n\nAdditionally, the time taken to fit the model was quite reasonable for the sample size (500):\n\ncopula_discrete_fit$time()$total\n\n[1] 290.1294\n\n\n\n\n\n\nCombining both continuous and discrete marginals is a simple combination of the two approaches, where the Copula function \\(C(\\cdot)\\) now includes both the continuous and discrete components while marginalising over the \\(u\\) for the discrete components.\nTo illustrate, consider if we extended the bivariate Poisson example to include two continuous outcomes \\(x_3\\) & \\(x_4\\) with arbitrary density functions \\(f_3(\\cdot)\\) & \\(f_4(\\cdot)\\) and marginal CDFs \\(F_3(\\cdot)\\) & \\(F_4(\\cdot)\\).\nThe joint density function would now be given as:\n\\[\nf(x_1, x_2, x_3, x_4) = \\\\\n\\]\n\\[\nf_3(x_3) \\cdot f_4(x_4) \\cdot \\sum_{j_1=1}^2 \\sum_{j_2=1}^2 (-1)^{j_1 + j_2} C(u_{1,j_1}, u_{2,j_2}, F_3(x_3), F_4(x_4) | \\Gamma)\n\\]\nOr, given that we are using the data-augmentation approach for the discrete marginals:\n\\[\nu_1 \\sim U\\left(F_1(x_1^-), F_1(x_1)\\right)\n\\]\n\\[\nu_2 \\sim U\\left(F_2(x_2^-), F_2(x_2)\\right)\n\\]\n\\[\nf(x_1, x_2, x_3, x_4) = f_3(x_x) \\cdot f_4(x_4) \\cdot c(u_1, u_2, F_3(x_3), F_4(x_4) | \\Gamma)\n\\]\nClearly, this allows for the trivial modelling of any combination of continuous and discrete marginals.\nTo illustrate, we will combine the previous two approaches, and estimate a Gaussian copula with marginals: \\[\ny_1 \\sim \\text{Exponential}(\\lambda)\n\\]\n\\[\ny_2 \\sim \\text{Chi-Square}(\\nu)\n\\]\n\\[\ny_3 \\sim \\text{Poisson}(\\lambda_r)\n\\]\n\\[\ny_4 \\sim \\text{Binomial}(N, \\theta_r)\n\\]\nAs with the previous examples, we will first generate the correlated outcomes:\n\ngamma &lt;- randcorr::randcorr(4)\nz &lt;- matrix(rnorm(2000), ncol = 4) %*% chol(gamma)\ny_mix &lt;- pnorm(z)\ny_mix[,1] &lt;- qexp(y_mix[,1], rate = lambda)\ny_mix[,2] &lt;- qchisq(y_mix[,2], df = nu)\n\nymix_denom &lt;- sample(1:100, 500, replace = TRUE)\ny_mix[,3] &lt;- qpois(y_mix[,3], lambda_r)\ny_mix[,4] &lt;- qbinom(y_mix[,4], size = ymix_denom, prob = theta_r)\n\nAnd update our Stan model simply append the bounded uniform parameters (for the discrete marginals) to the evaluations of the marginal CDFs for the continuous marginals, before passing these to the copila density function:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(array[] int pois_y, array[] int binom_y,\n                     array[] int binom_N, real lambda, real theta,\n                     int is_upper) {\n    int N = size(pois_y);\n    matrix[N, 2] u_bounds;\n\n    for (n in 1:N) {\n      if (is_upper == 0) {\n        u_bounds[n, 1] = pois_y[n] == 0.0\n                          ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n        u_bounds[n, 2] = binom_y[n] == 0.0\n                          ? 0.0 : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[n, 1] = poisson_cdf(pois_y[n] | lambda);\n        u_bounds[n, 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n  array[N] int pois_y;\n  array[N] int binom_y;\n  array[N] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda_exp;\n  real&lt;lower=0&gt; nu;\n  real&lt;lower=0&gt; lambda_pois;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(pois_y, binom_y, binom_N, lambda_pois, theta, 0),\n    upper=uvar_bounds(pois_y, binom_y, binom_N, lambda_pois, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[4] rho_chol;\n}\n\nmodel {\n  matrix[N, 4] u_mix;\n  for (n in 1:N) {\n    u_mix[n, 1] = exponential_cdf(Y[n,1] | lambda_exp);\n    u_mix[n, 2] = chi_square_cdf(Y[n,2] | nu);\n    u_mix[n, 3] = u[n, 1];\n    u_mix[n, 4] = u[n, 2];\n  }\n\n  Y[, 1] ~ exponential(lambda_exp);\n  Y[, 2] ~ chi_square(nu);\n  u_mix ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  corr_matrix[4] rho = multiply_lower_tri_self_transpose(rho_chol);\n}\n\n\ncopula_mix_mod &lt;- cmdstan_model(\"copula_mix.stan\")\n\n\ncopula_mix_fit &lt;- copula_mix_mod$sample(\n                                data = list(N = nrow(y_mix),\n                                            Y = y_mix[,1:2],\n                                            pois_y = y_mix[,3],\n                                            binom_y = y_mix[,4],\n                                            binom_N = ymix_denom),\n                                parallel_chains = 4)\n\nWe can see that the data-generating parameters (both for the marginals and dependence between them) have all been successfully recovered\n\ncopula_mix_fit$summary(variables = c(\"lambda_exp\", \"nu\", \"lambda_pois\", \"theta\"))\n\n# A tibble: 4 × 10\n  variable     mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda_exp  2.05   2.05  0.0788  0.0799  1.93   2.18   1.00    8062.    2571.\n2 nu          4.97   4.97  0.0878  0.0891  4.83   5.12   1.00    4338.    3371.\n3 lambda_pois 9.94   9.94  0.140   0.142   9.71  10.2    1.00   10417.    3086.\n4 theta       0.701  0.701 0.00161 0.00158 0.698  0.703  1.00    4280.    3131.\n\n\n\ngamma\n\n            [,1]       [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.4562896  0.02669022 -0.69232431\n[2,] -0.45628959  1.0000000 -0.10901915  0.92526889\n[3,]  0.02669022 -0.1090192  1.00000000  0.06522568\n[4,] -0.69232431  0.9252689  0.06522568  1.00000000\n\ncopula_mix_fit$summary(variables = c(\"rho\"))\n\n# A tibble: 16 × 10\n   variable     mean   median      sd     mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 rho[1,1]  1        1       0       0        1        1      NA         NA \n 2 rho[2,1] -0.431   -0.431   0.0262  0.0266  -0.473   -0.387   1.00    4173.\n 3 rho[3,1]  0.0758   0.0760  0.0416  0.0421   0.00660  0.144   1.00    8283.\n 4 rho[4,1] -0.679   -0.679   0.0176  0.0181  -0.708   -0.650   1.00    4176.\n 5 rho[1,2] -0.431   -0.431   0.0262  0.0266  -0.473   -0.387   1.00    4173.\n 6 rho[2,2]  1        1       0       0        1        1      NA         NA \n 7 rho[3,2] -0.153   -0.153   0.0392  0.0389  -0.216   -0.0879  1.00    5701.\n 8 rho[4,2]  0.923    0.923   0.00466 0.00481  0.915    0.930   1.00    6795.\n 9 rho[1,3]  0.0758   0.0760  0.0416  0.0421   0.00660  0.144   1.00    8283.\n10 rho[2,3] -0.153   -0.153   0.0392  0.0389  -0.216   -0.0879  1.00    5701.\n11 rho[3,3]  1        1       0       0        1        1      NA         NA \n12 rho[4,3]  0.00589  0.00578 0.0403  0.0390  -0.0603   0.0718  1.00    5912.\n13 rho[1,4] -0.679   -0.679   0.0176  0.0181  -0.708   -0.650   1.00    4176.\n14 rho[2,4]  0.923    0.923   0.00466 0.00481  0.915    0.930   1.00    6795.\n15 rho[3,4]  0.00589  0.00578 0.0403  0.0390  -0.0603   0.0718  1.00    5912.\n16 rho[4,4]  1        1       0       0        1        1      NA         NA \n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\ncopula_mix_fit$time()$total\n\n[1] 498.0359\n\n\n\n\n\nThe use of data augmentation with discrete marginals also allows for the trivial handling of missing data on discrete outcomes. While missing continuous data in Stan models can easily by handled by estimating the observation as a parameter in the model, the same approach cannot be used for discrete data - as HMC-based methods like Stan cannot estimate discrete parameters. However, as the data-augmented copula model is already estimating a uniform parameter for each discrete observation, we can simply set the bounds for the uniform parameter to \\((0,1)\\) (as we do not have an observed CDF for the bounds).\nAs such, the uniform parameters are instead estimated as:\n\\[\nu_m \\sim \\begin{cases}\n  U\\left(F_m(x_m^-), F_m(x_m)\\right) & \\text{if } x_m \\text{ is observed} \\\\\n  U(0, 1) & \\text{if } x_m \\text{ is missing}\n\\end{cases}\n\\]\nThis is also trivial to integrate into our existing Stan model. We simply update our bounds function (uvar_bounds()) to set the bounds to \\((0,1)\\) by default, and then set the bounds to \\((F_m(x_m^-), F_m(x_m))\\) for the observed outcomes:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(int N, array[] int pois_y, array[] int binom_y,\n                      array[] int binom_N, array[] int pois_y_ind,\n                      array[] int binom_y_ind, real lambda, real theta,\n                      int is_upper) {\n    matrix[N, 2] u_bounds = rep_matrix(is_upper, N, 2);\n\n    for (n in 1:size(pois_y_ind)) {\n      if (is_upper == 0) {\n        u_bounds[pois_y_ind[n], 1] = pois_y[n] == 0.0\n                                      ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n      } else {\n        u_bounds[pois_y_ind[n], 1] = poisson_cdf(pois_y[n] | lambda);\n      }\n    }\n\n    for (n in 1:size(binom_y_ind)) {\n      if (is_upper == 0) {\n        u_bounds[binom_y_ind[n], 2] = binom_y[n] == 0.0 ? 0.0\n                                      : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[binom_y_ind[n], 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; Npois;\n  int&lt;lower=0&gt; Nbinom;\n  matrix[N, 2] Y;\n  array[Npois] int pois_y;\n  array[Npois] int pois_y_ind;\n  array[Nbinom] int binom_y;\n  array[Nbinom] int binom_y_ind;\n  array[Nbinom] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda_exp;\n  real&lt;lower=0&gt; nu;\n  real&lt;lower=0&gt; lambda_pois;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(N, pois_y, binom_y, binom_N, pois_y_ind,\n                      binom_y_ind, lambda_pois, theta, 0),\n    upper=uvar_bounds(N, pois_y, binom_y, binom_N, pois_y_ind,\n                      binom_y_ind, lambda_pois, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[4] rho_chol;\n}\n\nmodel {\n  matrix[N, 4] u_mix;\n  for (n in 1:N) {\n    u_mix[n, 1] = exponential_cdf(Y[n,1] | lambda_exp);\n    u_mix[n, 2] = chi_square_cdf(Y[n,2] | nu);\n    u_mix[n, 3] = u[n, 1];\n    u_mix[n, 4] = u[n, 2];\n  }\n\n  Y[, 1] ~ exponential(lambda_exp);\n  Y[, 2] ~ chi_square(nu);\n\n  u_mix ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  corr_matrix[4] rho = multiply_lower_tri_self_transpose(rho_chol);\n}\n\nTo test this, we will randomly remove 10% of the data for each of the Poisson and Binomial outcomes and fit the model:\n\ny_mix_miss &lt;- y_mix\ny_mix_miss[sample(1:500, 50), 3] &lt;- NA\ny_mix_miss[sample(1:500, 50), 4] &lt;- NA\npois_y_ind &lt;- which(!is.na(y_mix_miss[, 3]))\nbinom_y_ind &lt;- which(!is.na(y_mix_miss[, 4]))\n\nstandata &lt;-  list(N = nrow(y_mix_miss),\n                        Npois = length(pois_y_ind),\n                        Nbinom = length(binom_y_ind),\n                        Y = y_mix_miss[, 1:2],\n                        pois_y = y_mix_miss[pois_y_ind, 3],\n                        pois_y_ind = pois_y_ind,\n                        binom_y = y_mix_miss[binom_y_ind, 4],\n                        binom_y_ind = binom_y_ind,\n                        binom_N = ymix_denom[binom_y_ind])\n\n\ncopula_mix_missing_mod &lt;- cmdstan_model(\"copula_mix_missing.stan\")\n\n\ncopula_mix_missing_fit &lt;- copula_mix_missing_mod$sample(data = standata, parallel_chains = 4)\n\nWe can see that even with missing discrete data, the data-generating parameters have been successfully recovered:\n\ncopula_mix_missing_fit$summary(variables = c(\"lambda_exp\", \"nu\", \"lambda_pois\", \"theta\"))\n\n# A tibble: 4 × 10\n  variable      mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda_exp   2.06   2.06  0.0810  0.0806  1.93   2.20   1.00    6094.    2952.\n2 nu           4.98   4.98  0.0908  0.0882  4.83   5.14   1.00    2905.    2946.\n3 lambda_pois 10.0   10.0   0.148   0.151   9.79  10.3    1.00   10154.    2937.\n4 theta        0.701  0.701 0.00170 0.00166 0.698  0.704  1.00    2827.    3106.\n\n\n\ngamma\n\n            [,1]       [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.4562896  0.02669022 -0.69232431\n[2,] -0.45628959  1.0000000 -0.10901915  0.92526889\n[3,]  0.02669022 -0.1090192  1.00000000  0.06522568\n[4,] -0.69232431  0.9252689  0.06522568  1.00000000\n\ncopula_mix_missing_fit$summary(variables = c(\"rho\"))\n\n# A tibble: 16 × 10\n   variable     mean   median      sd     mad      q5     q95   rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 rho[1,1]  1        1       0       0        1       1      NA          NA \n 2 rho[2,1] -0.432   -0.432   0.0260  0.0261  -0.474  -0.390   1.00     5714.\n 3 rho[3,1]  0.0854   0.0861  0.0419  0.0420   0.0150  0.154   1.000    8959.\n 4 rho[4,1] -0.679   -0.679   0.0176  0.0174  -0.707  -0.649   1.00     5144.\n 5 rho[1,2] -0.432   -0.432   0.0260  0.0261  -0.474  -0.390   1.00     5714.\n 6 rho[2,2]  1        1       0       0        1       1      NA          NA \n 7 rho[3,2] -0.164   -0.165   0.0406  0.0414  -0.230  -0.0972  1.00     5941.\n 8 rho[4,2]  0.923    0.924   0.00488 0.00491  0.915   0.931   1.00     5662.\n 9 rho[1,3]  0.0854   0.0861  0.0419  0.0420   0.0150  0.154   1.000    8959.\n10 rho[2,3] -0.164   -0.165   0.0406  0.0414  -0.230  -0.0972  1.00     5941.\n11 rho[3,3]  1        1       0       0        1       1      NA          NA \n12 rho[4,3] -0.00876 -0.00858 0.0416  0.0425  -0.0767  0.0589  1.00     5934.\n13 rho[1,4] -0.679   -0.679   0.0176  0.0174  -0.707  -0.649   1.00     5144.\n14 rho[2,4]  0.923    0.924   0.00488 0.00491  0.915   0.931   1.00     5662.\n15 rho[3,4] -0.00876 -0.00858 0.0416  0.0425  -0.0767  0.0589  1.00     5934.\n16 rho[4,4]  1        1       0       0        1       1      NA          NA \n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\n\nLeave-one-out cross-validation (LOO-CV) is a popular and powerful tool for model comparison and selection. However, it can be difficult to compute for likelihoods which cannot be factorised into a product of individual likelihoods. Thankfully, Buerkner, Gabry, and Vehtari (2020) have previously derived a method for calculating approximate LOO-CV for non-factorisable models with a multivariate-normal outcome distribution.\n\n\nGiven a vector \\(y\\) which follows a multivariate-normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\):\n\\[\ny \\sim \\mathcal{MVN}(\\mu, \\Sigma)\n\\]\nThe pointwise likelihood is given by:\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(y_i | y_i - \\frac{g_i}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}})\n\\]\nWhere:\n\\[\ng_i = [\\Sigma^{-1}(y - \\mu)]_i\n\\]\n\\[\n\\bar{\\sigma}_{ii} = [\\Sigma^{-1}]_{ii}\n\\]\n\n\n\nAs demonstrated earlier, the Gaussian copula density can be expressed as a multivariate normal density divided by the product of the univariate standard normal densities. As such, we can calculate the approximate LOO-CV for a Gaussian copula by applying Buerkner, Gabry, and Vehtari (2020)’s method to the multivariate normal density to obtain a pointwise likelihood, and then divide by the respective univariate standard normal density.\nTo summarise, given that a Gaussian copula with continuous marginals has density function:\n\\[\nf(y) = c(\\Phi(y) | \\Gamma) \\prod_{i=1}^{m} f_i(y_i)\n\\]\n\\[\nc(\\Phi(y) | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\prod_{i=1}^m\\textrm{N}(q_i | 0, 1)^{-1}\n\\]\nThe pointwise conditional likelihood is given by:\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(q_i | q_i - \\frac{g_i}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}}) \\cdot \\textrm{N}(q_i | 0, 1)^{-1} \\cdot f_i(y_i)\n\\]\nWe can then use the PSIS algorithm implemented in the loo package to approximate \\(p(y_i | y_{-i})\\).\n\n\n\nIn order to validate the approximation, we can also use the method proposed by Buerkner at. al (2020) to perform exact LOO-CV. To do so, the held-out observation \\(y_i\\) is estimated as a parameter in the model \\(y_i^{miss}\\) and substituted into the set of observations \\(y\\) before calculating the marginal CDFs:\n\\[\nu_{miss(i)} = \\left(F_1(y_1),...,F_{i-1}(y_{i-1}),F_i(y_i^{miss}),F_{i+1}(y_{i+1}),..., F_m(y_m)\\right)\n\\] Such that \\(q_{miss(i)}\\) is then the result of the standard normal quantile function for each element of \\(u_{miss(i)}\\). This new \\(q_{miss(i)}\\) set is then used to define the parameters for the log-predictive density when evaluating it the held-out observation \\(q_i\\):\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(q_i | q_i^{miss} - \\frac{g_i^{miss}}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}}) \\cdot \\textrm{N}(q_i | 0, 1)^{-1} \\cdot f_i(y_i)\n\\] Where: \\[\ng_i^{miss} = [\\Gamma^{-1}(q_{miss(i)})]_i\n\\] And we can then estimate the leave-one-out density for the held-out observation using the posterior samples:\n\\[\np(y_i | y_{-i}) = \\sum_{s=1}^{S} p(y_i | y_{-i}, \\theta_{-i}^{(s)})\n\\]\n\n\n\nTo implement these, we will update our Stan model to either calculate the exact LOO density for a held-out observation (and estimate that observation as a parameter), or to calculate the density for all observations for use in approximate LOO-CV estimation.\nWe add a data argument y_miss_i to the model to specify the index of the missing observation. If y_miss_i is 0, the model will calculate the approximate LOO-CV density for all observations. If y_miss_i is greater than 0, the model will calculate the exact LOO-CV density for the observation at index y_miss_i.\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  row_vector gauss_copula_cholesky_pointwise(row_vector y_obs, row_vector y_miss,\n                                              real lambda, real nu, matrix Sigma_inv) {\n    int J = cols(Sigma_inv);\n    vector[J] inv_sigma_inv = inv(diagonal(Sigma_inv));\n\n    row_vector[J] log_lik;\n    vector[2] u_miss = [ exponential_cdf(y_miss[1] | lambda),\n                          chi_square_cdf(y_miss[2] | nu) ]';\n    vector[2] u = [ exponential_cdf(y_obs[1] | lambda),\n                    chi_square_cdf(y_obs[2] | nu) ]';\n    vector[2] q_miss = inv_Phi(u_miss);\n    vector[2] q = inv_Phi(u);\n    vector[2] g = Sigma_inv * q_miss;\n\n    log_lik = [ exponential_lpdf(y_obs[1] | lambda),\n                      chi_square_lpdf(y_obs[2] | nu) ];\n\n    for (j in 1:J) {\n      log_lik[j] += normal_lpdf(q[j] | q_miss[j] - g[j] * inv_sigma_inv[j],\n                                        sqrt(inv_sigma_inv[j]))\n                      - std_normal_lpdf(q[j]);\n    }\n    return log_lik;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n  int y_miss_i;\n}\n\ntransformed data {\n  int has_missing = y_miss_i &gt; 0;\n  int y_miss_col = (y_miss_i - 1) %/% N + 1;\n  int y_miss_row = y_miss_i - (y_miss_col - 1) * N;\n  array[2] int y_miss_idx = { y_miss_row, y_miss_col };\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0&gt; nu;\n  cholesky_factor_corr[2] rho_chol;\n  array[has_missing] real&lt;lower=0&gt; y_miss;\n}\n\nmodel {\n  matrix[N, 2] u;\n  matrix[N, 2] Ymiss = Y;\n  if (has_missing) {\n    Ymiss[y_miss_idx[1], y_miss_idx[2]] = y_miss[1];\n  }\n\n  for (n in 1:N) {\n    u[n, 1] = exponential_cdf(Ymiss[n, 1] | lambda);\n    u[n, 2] = chi_square_cdf(Ymiss[n, 2] | nu);\n  }\n\n  Ymiss[, 1] ~ exponential(lambda);\n  Ymiss[, 2] ~ chi_square(nu);\n\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n  vector[has_missing ? 1 : N*2] log_lik;\n\n  {\n    int J = 2;\n    matrix[J,J] Sigma_inv = chol2inv(rho_chol);\n    matrix[has_missing ? 1 : N, J] log_lik_mat;\n    array[has_missing ? 1 : N] int iters = has_missing ? {y_miss_idx[1]}\n                                            : linspaced_int_array(N, 1, N);\n\n    for (n in iters) {\n      row_vector[J] Ymiss = Y[n];\n      if (has_missing) {\n        Ymiss[y_miss_idx[2]] = y_miss[1];\n      }\n      log_lik_mat[has_missing ? 1 : n]\n          = gauss_copula_cholesky_pointwise(Y[n], Ymiss, lambda, nu, Sigma_inv);\n    }\n\n    if (has_missing) {\n      log_lik[1] = log_lik_mat[1, y_miss_idx[2]];\n    } else {\n      log_lik = to_vector(log_lik_mat);\n    }\n  }\n}\n\n\ncopula_mod_loo &lt;- cmdstan_model(\"copula_mod_loo.stan\")\n\nFirst we’ll estimate the approximate LOO-CV for all observations:\n\ncopula_loo_fit &lt;- copula_mod_loo$sample(data = list(N = nrow(y), Y = y, y_miss_i = 0),\n                                parallel_chains = 4)\napprox_loo &lt;- copula_loo_fit$loo()\n\nNext, we’ll estimate the exact LOO-CV for each observation. Given the number of observations to hold-out (1000), we will do this parallel using the furrr and future packages:\n\nfuture::plan(future::multisession)\n\nrun_fun &lt;- function(idx) {\n  copula_mod &lt;- cmdstanr::cmdstan_model(exe_file = copula_mod_loo$exe_file())\n  copula_fit_iter &lt;- copula_mod$sample(\n    data = list(N = nrow(y), Y = y, y_miss_i = idx)\n  )\n\n  log_lik_draws &lt;- copula_fit_iter$draws(variables = \"log_lik\", format = \"draws_df\")$`log_lik[1]`\n\n  data.frame(res = as.numeric(log_lik_draws)) |&gt;\n    setNames(paste0(\"log_lik_\", idx))\n}\n\nexact_lpd &lt;- furrr::future_map_dfc(1:1000, run_fun, .progress = TRUE)\nlog_mean_exp &lt;- function(x) {\n  # more stable than log(mean(exp(x)))\n  max_x &lt;- max(x)\n  max_x + log(sum(exp(x - max_x))) - log(length(x))\n}\nexact_elpds &lt;- apply(exact_lpd, 2, log_mean_exp)\n\nfuture::plan(future::sequential)\n\nThen we can visually compare the approximate and exact LOO-CV estimates:\n\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(bayesplot))\ncolor_scheme_set(\"brightblue\")\ntheme_set(theme_default())\n\n# Create a summary annotation for the plot\nplot_lab &lt;- paste0(paste0(\"Exact ELPD: \", round(sum(exact_elpds), 2)), \"\\n\",\n              paste0(\"Approx ELPD: \", round(sum(approx_loo$pointwise[,1]), 2)))\n\ndata.frame(loo_elpd = approx_loo$pointwise[,1],\n           exact_lpd = exact_elpds) |&gt;\n  ggplot(aes(x = loo_elpd, y = exact_lpd)) +\n    geom_abline(color = \"gray30\") +\n    geom_point(size = 2) +\n    xlab(\"Approximate elpds\") +\n    ylab(\"Exact elpds\") +\n    annotate(\"text\", x = 0, y = -6, label = plot_lab)\n\n\n\n\n\n\n\n\nWe can see that the exact and approximate LOO-CV estimates are extremely consistent.",
    "crumbs": [
      "Case Studies",
      "Copulas"
    ]
  },
  {
    "objectID": "CopulaIntro.html#an-introduction-to-copula-modelling",
    "href": "CopulaIntro.html#an-introduction-to-copula-modelling",
    "title": "Copulas",
    "section": "",
    "text": "A common goal is the modelling of dependence between outcomes, however this is only trivially/commonly implemented for Gaussian or Student-T distributed outcomes.\nAn alternative, more flexible approach is to model the dependence structure separately from the marginal distributions. This is the idea behind copula modelling.\n\n\nA copula function allows for the construction of a multivariate cumulative distribution function (CDF) from a set of univariate CDFs (for each marginal), with no requirement that the marginals are Gaussian or Student-T distributed - or that they all have the same distribution.\nGiven a set of outcomes \\(x_1, ..., x_m\\), with marginal CDFs \\(F_1, ..., F_m\\), and the results of applying these CDFs to the outcomes \\(u_1 = F_1(x_1), ..., u_m = F_m(x_m)\\), the copula function \\(C\\) is defined as:\n\\[\nC(u | \\Gamma) = G_m(G^{-1}(u_1), ..., G^{-1}(u_m)|\\Gamma)\n\\] Where \\(G_m\\) is an \\(m\\)-variate CDF for a given distribution, \\(G^{-1}\\) is the univariate quantile function (inverse-CDF) for that distribution, and \\(\\Gamma\\) is a correlation matrix.\nThe most popular implementation of this is the Gaussian copula:\n\\[\nC(u | \\Gamma) = \\Phi_m(\\Phi^{-1}(u_1), ..., \\Phi^{-1}(u_m)|\\Gamma)\n\\] Where \\(\\Phi_m\\) is the \\(m\\)-variate CDF for the standard normal distribution, and \\(\\Phi^{-1}\\) is the univariate quantile function for the standard normal distribution.\n\n\nNote that correlation matrix \\(\\Gamma\\) can no longer be interpreted on the scale of the observed outcomes. Instead, these correlations are now on the scale of the copula function. For example, when a Gaussian copula is used with continuous marginals the estimated correlation is linear between normal scores:\n\\[\n\\Gamma_{1,2} = \\text{Corr}(\\Phi^{-1}(F_1(x_1)), \\Phi^{-1}(F_2(x_2)))\n\\]\nWhen a Gaussian copula is used with a discrete marginal, the correlation is with a normally-distributed latent variate. For example, with two ordinal outcomes, the correlation would be interpreted as a polychoric correlation coefficient. This can also be used as a motivating reason for using the Gaussian copula, as opposed to other forms, as the estimated correlations have existing analogues in the literature with known interpretations.\n\n\n\n\nThe Copula approach to generating random variates also allows for the very simple generation of correlated/dependent data, regardless of their desired marginal distributions. To this, we essentially reverse the steps involved in a Gaussian copula - instead of transforming the observed outcomes to multivariate normal scores, we transform multivariate normal scores to the desired distributions.\n\\[\n\\begin{bmatrix}z_1\\\\ \\vdots \\\\ z_m \\end{bmatrix}\n\\sim \\textrm{MVN}\n\\left(\n  \\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\end{bmatrix},\n  \\begin{bmatrix}\n    1 & \\dots & \\rho_{1m} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{m1} & \\dots & 1\n  \\end{bmatrix}\n\\right)\n\\]\n\\[\n\\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_m \\end{bmatrix} = \\begin{bmatrix}F_1^{-1}(\\Phi(z_1))\\\\ \\vdots \\\\ F_m^{-1}(\\Phi(z_m)) \\end{bmatrix}\n\\]\nWhere \\(F_i^{-1}(\\cdot)\\) is the quantile function for the \\(i\\)-th marginal distribution, and \\(\\Phi(\\cdot)\\) is the standard normal CDF.\nThis can be further simplified by using the Cholesky decomposition of the correlation matrix to construct the multivariate normal variates:\n\\[\nx \\sim \\textrm{N(0, 1)} \\Rightarrow z = Lx \\sim \\textrm{MVN}(0, LL^T)\n\\]\nWhere \\(L\\) is the Cholesky decomposition of the correlation matrix \\(\\Gamma\\).\nTo illustrate, consider the goal of generating an exponentially-distributed outcome \\(y_1\\) (with rate parameter \\(\\lambda\\)) and a chi-squared-distributed outcome \\(y_2\\) (with degrees of freedom \\(\\nu\\)) with a correlation of 0.5. This can be simply achieved by sampling from a univariate standard-normal, multiplying by the cholesky factor of the desired correlations, and then transforming these to the desired distributions:\n\nset.seed(2024)\nlambda &lt;- 2\nnu &lt;- 5\n\nL &lt;- chol(matrix(c(1, 0.5, 0.5, 1), nrow = 2))\nz &lt;- matrix(rnorm(1000), ncol = 2) %*% L\n\ny &lt;- pnorm(z)\ny[,1] &lt;- qexp(y[,1], rate = lambda)\ny[,2] &lt;- qchisq(y[,2], df = nu)\n\n\n\n\nWith the copula function defined, the joint density function with all continuous marginals is given as:\n\\[\nf(x) = c(u | \\Gamma) \\prod_{i=1}^{m} f_i(x_i)\n\\] Where \\(c(\\dot)\\) is the density function for the chosen copula function and \\(f_i\\) is the marginal density function for the \\(i\\)-th outcome.\n\n\nFor a Gaussian copula, the density function is given as:\n\\[\nc(u | \\Gamma) = |\\Gamma|^{-1/2} \\exp\\left(\\frac{q^T\\left(I_m - \\Gamma^{-1}\\right)q}{2}\\right)\n\\] \\[\n= |\\Gamma|^{-1/2} \\exp\\left(\\frac{q^Tq}{2} - \\frac{q^T\\Gamma^{-1}q}{2}\\right)\n\\]\nWhere \\(q = \\Phi^{-1}(u)\\).\nNote that when the outcomes are independent (i.e., \\(\\Gamma = I\\)), or there is only a single outcome, the joint density function simplifies to the product of the marginal density functions.\n\n\nThe form of the Gaussian copula density above is also now amenable to being expressed using a multivariate normal. Where the multivariate standard normal density is given as:\n\\[\n\\textrm{MVN}(q | 0, \\Gamma) = \\left(2\\pi\\right)^{-m/2} |\\Gamma|^{-1/2} \\exp\\left(-\\frac{q^T\\Gamma^{-1}q}{2}\\right)\n\\]\nThe Gaussian copula density can be expressed as a multivariate standard-normal density multipled by an adjustment term:\n\\[\nc(u | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\exp\\left(\\frac{q^Tq}{2}\\right) \\cdot \\left(2\\pi\\right)^{m/2}\n\\]\nThe adjustment term above is equal to the reciprocal of the standard-normal density evaluated at \\(q\\), giving the final form of the Gaussian copula density:\n\\[\nc(u | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\prod_{i=1}^m\\textrm{N}(q_i | 0, 1)^{-1}\n\\]\nThis form also makes intuitive sense. The Gaussian copula is specifying a multivariate normal density over a transformation of inputs (\\(q = \\Phi(u)\\)), so we need to adjust the density by the absolute derivative of the transformation to account for this. The derivative of the standard normal CDF \\(\\Phi()\\) is the reciprocal of the standard normal density, so we simply multiply the multivariate normal density by the product of these reciprocals.\nThis also provides other benefits, as we can simply delegate to existing implementations of these density functions which have already received significant optimisation and testing.\n\n\n\n\nTo illustrate, we can fit a bivariate Gaussian copula to the generated data from the previous example to see whether we can recover the data-generating parameters. We will first show to estimate these using simple maximum likelihood estimation in R, and then using a Bayesian approach.\nTo refresh, we have an exponentially-distributed outcome \\(y_1\\) with rate parameter \\(\\lambda\\), a chi-squared-distributed outcome \\(y_2\\) with degrees of freedom \\(\\nu\\), and we are modelling the dependence between these outcomes using a Gaussian copula with correlation \\(\\rho\\):\n\\[\ny_1 \\sim \\textrm{Exp}(\\lambda)\n\\]\n\\[\ny_2 \\sim \\chi^2(\\nu)\n\\]\n\\[\n\\begin{bmatrix}F_1(y_1) \\\\ F_2(y_2) \\end{bmatrix} \\sim \\textrm{c}_{\\Phi}\n\\left(\n  \\begin{bmatrix}\n    1 & \\rho \\\\\n    \\rho & 1\n  \\end{bmatrix}\n\\right)\n\\] Where \\(F_i(\\cdot)\\) is the CDF for the \\(i\\)-th marginal distribution, and \\(\\textrm{c}_{\\Phi}(\\cdot)\\) is the Gaussian copula density.\n\n\n\nTo estimate these parameters via maximum-likelihood in R, we need to define a function that takes a vector of parameter values and returns the joint log-likelihood. It is also useful to define the function such that the input parameters can take on any value, and then transform these to the appropriate scale within the function (i.e., \\(\\rho \\in \\left(-1,1\\right)\\)). Given that we will then be using transformations of the parameters to define the log-likelihood, we need to add a correction factor to account for this. For more information and examples of different transformations & corrections, see this Stan Documentation Page.\nFirst, we will define a function to calculate the gaussian copula density for a given set of uniform variates \\(u\\) and correlation matrix \\(\\Gamma\\):\n\ndgauss_copula &lt;- function(u, gamma, log = TRUE) {\n  # Map uniform variates to normal scores\n  q &lt;- qnorm(u)\n  ll &lt;- sum(RTMB::dmvnorm(q, Sigma = gamma, log = TRUE)) - sum(dnorm(q, log = TRUE))\n  ifelse(isTRUE(log), ll, exp(ll))\n}\n\n\n# Define the log-likelihood function\nll_function &lt;- function(parameters, data_y) {\n  # Constrain input parameters to valid values for distributions\n  lambda &lt;- exp(parameters[1])\n  nu &lt;- exp(parameters[2])\n  rho &lt;- tanh(parameters[3])\n\n  adjustments &lt;- sum(parameters[1:2]) + log1p(-(rho*rho))\n\n  marginal_log_lik &lt;- sum(dexp(data_y[,1], rate = lambda, log = TRUE)) +\n                      sum(dchisq(data_y[,2], df = nu, log = TRUE))\n\n  # Apply marginal CDFs to data\n  u &lt;- data_y\n  u[,1] &lt;- pexp(data_y[,1], rate = lambda)\n  u[,2] &lt;- pchisq(data_y[,2], df = nu)\n\n  copula_log_lik &lt;- dgauss_copula(u, matrix(c(1, rho, rho, 1), nrow = 2),\n                                  log = TRUE)\n\n  # Return joint log-likelihood with adjustments for change-of-variables\n  marginal_log_lik + copula_log_lik + adjustments\n}\n\nHowever, rather than returning the log-likelihood itself, we need to return the negative log-likelihood. This is because the optimisation functions in R are minimisation functions. To do this, we’ll simply create a ‘wrapper’ function which takes an arbitrary function as input and returns a function which itseld returns the negative of the input function’s output:\n\nneg_ll_function &lt;- function(f) {\n  function(...) { -f(...) }\n}\n\nNext we can use the optim function to estimate the parameters:\n\nresult &lt;- optim(\n  par = c(0, 0, 0), # initial values for parameters\n  fn = neg_ll_function(ll_function),\n  data_y = y\n)\n\nAnd extract and transform them to the appropriate scale, showing that the recovery was successful:\n\nc(\n  \"lambda\" = exp(result$par[1]),\n  \"nu\" = exp(result$par[2]),\n  \"rho\" = tanh(result$par[3])\n) |&gt; round(2)\n\nlambda     nu    rho \n  1.96   4.91   0.54 \n\n\n\n\n\nWe can also estimate these parameters using a Bayesian approach. This can be done in two ways: by using our existing ll_function() implementation for sampling, or by writing a Stan model to estimate the parameters.\n\n\nEstimating the posterior distributions for parameters of an R function can be done quite simply using the StanEstimators R package:\n\nif (!requireNamespace(\"StanEstimators\", quietly = TRUE)) {\n  remotes::install_github(\"andrjohns/StanEstimators\")\n}\nlibrary(StanEstimators)\nsuppressPackageStartupMessages(library(posterior))\n\nThis will allow us to use algorithms implemented by Stan to estimate parameters for arbitrary R functions. To use full Bayesian sampling with the No-U-Turn Sampler (NUTS) algorithm, we can use the stan_sample() function:\n\n# Record executtion for later comparison\nsampling_time &lt;- system.time({\n  results_stan &lt;- stan_sample(fn = ll_function,\n                              par_inits = c(0,0,0),\n                              grad_fun = \"RTMB\",\n                              additional_args = list(data_y = y),\n                              parallel_chains = 4,\n                              quiet = TRUE)\n})\n\nError in (function (parameters, data_y) : unused argument (v = new(\"advector\", , c(0+4.6421248212267e-310i, 4.94065645841247e-324+4.6421248212267e-310i, 9.88131291682493e-324+4.6421248212267e-310i)))\n\n\nNext, we can use the posterior package to extract the posterior draws for the parameters and transform them to the appropriate scale before summarising:\n\nresults_stan@draws |&gt;\n    mutate_variables(lambda = exp(`pars[1]`),  nu = exp(`pars[2]`),  rho = tanh(`pars[3]`)) |&gt;\n    subset_draws(variable=c(\"lambda\", \"nu\", \"rho\")) |&gt;\n    summarise_draws()\n\nError: object 'results_stan' not found\n\n\nWe can see that estimates are consistent with their maximum likelihood counterparts, and the narrow credible intervals and high effective sample sizes indicate that we can have confidence in the estimated values.\nHowever, full Bayesian sampling from an R function can be computationally expensive and inefficient. This is especially true in this case where we have not also provided a provided a function to calculate the gradients for each parameter. Without this, StanEstimators has to fall back to using finite differences to estimate the gradients, which can be slow and inaccurate.\n\n\n\nOne alternative, before moving to using a Stan model directly, is to use the Pathfinder algorithm for approximate Bayesian inference. As the name implies, this allows us to approximate the results from full Bayesian sampling with dramatically reduced computational cost:\n\npath_time &lt;- system.time({\n  results_path &lt;- stan_pathfinder(fn = ll_function,\n                                  par_inits = c(0,0,0),\n                                  grad_fun = \"RTMB\",\n                                  additional_args = list(data_y = y),\n                                  quiet = TRUE,\n                            # Return same number of approximate draws as sampling\n                            num_psis_draws = 4000)\n})\n\nError in (function (parameters, data_y) : unused argument (v = new(\"advector\", , c(0+4.64212144286826e-310i, 4.94065645841247e-324+4.64212144286826e-310i, 9.88131291682493e-324+4.64212144286826e-310i)))\n\n\nWe can see that this provided posterior estimates consistent with sampling:\n\nresults_path@draws |&gt;\n    mutate_variables(lambda = exp(`pars[1]`),  nu = exp(`pars[2]`),  rho = tanh(`pars[3]`)) |&gt;\n    subset_draws(variable=c(\"lambda\", \"nu\", \"rho\")) |&gt;\n    summarise_draws()\n\nError: object 'results_path' not found\n\n\nWhile the execution time was orders of magnitude faster:\n\nc(\"Stan\" = sampling_time[3], \"Pathfinder\" = path_time[3])\n\nError: object 'sampling_time' not found\n\n\nFor more efficient Bayesian sampling, especially with larger or more complex models, it is recommended to use a Stan model directly.\n\n\n\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0&gt; nu;\n  cholesky_factor_corr[2] rho_chol;\n}\n\nmodel {\n  matrix[N, 2] u;\n\n  for (n in 1:N) {\n    u[n, 1] = exponential_cdf(Y[n, 1] | lambda);\n    u[n, 2] = chi_square_cdf(Y[n, 2] | nu);\n  }\n\n  Y[, 1] ~ exponential(lambda);\n  Y[, 2] ~ chi_square(nu);\n\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n}\n\nWe will be using the cmdstanr package for our model fitting:\n\nif (!requireNamespace(\"cmdstanr\", quietly = TRUE)) {\n  remotes::install_github(\"stan-dev/cmdstanr\")\n  cmdstanr::check_cmdstan_toolchain(fix = TRUE)\n}\n\nFirst, we create a model object from the above Stan code, which has been saved to a file named gauss_copula_continuous.stan:\n\ncopula_mod &lt;- cmdstan_model(\"gauss_copula_continuous.stan\")\n\nThen we can use the compiled model object to sample our parameters:\n\ncopula_fit &lt;- copula_mod$sample(data = list(N = nrow(y), Y = y),\n                                parallel_chains = 4)\n\nWe can see that the results are consistent with the estimates from our StanEstimators implementation:\n\ncopula_fit$summary(variables = c(\"lambda\", \"nu\", \"rho\"))\n\n# A tibble: 3 × 10\n  variable  mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda   1.96   1.96  0.0829 0.0842 1.83  2.10   1.00    2741.    3058.\n2 nu       4.91   4.91  0.120  0.118  4.72  5.11   1.00    2999.    2482.\n3 rho      0.536  0.538 0.0280 0.0274 0.487 0.580  1.00    2742.    2583.\n\n\nBut the time taken to fit the model is significantly faster:\n\ncopula_fit$time()$total\n\n[1] 10.76354\n\n\n\n\n\n\n\n\n\nWhile the use of copulas are attractive for continuous marginals, their application to discrete marginals is less straightforward. It was possible to analytically define the density function for continuous marginals as there was a unique, one-to-one mapping from the observed, marginal scale (\\(x_m\\)) to copula function scale (\\(q = \\Phi^{-1}(u)\\)). However, this is not the case for discrete marginals, as there are now a range of \\(u_m\\) values that map to the same \\(x_m\\) value.\nFor discrete marginals, we are instead estimating a probability mass function (PMF): summing the copula density function over all possible values of \\(u_m\\) that map to the observed \\(x_m\\) value.\nNote that the PMF for a given univariate discrete distribution can be expressed as:\n\\[\nf(x) = F(x) - F(x^-)\n\\] Where \\(x^-\\) denotes the previous value of \\(x\\) (or 0 if \\(x == 0\\)).\nThis is extended to the multivariate case by summing, for each \\(x_m\\), the difference between the copula function evaluated at \\(x_m\\) and \\(x_m^-\\), holding all other \\(x_i\\) constant. In other words, we need to integrate the uniform \\(u_m\\) variable out of the likelihood. This is expressed as:\n\\[\nf(x) = \\sum_{j_1=1}^2 \\cdot\\cdot\\cdot \\sum_{j_m=1}^2 (-1)^{j_1 + \\cdot\\cdot\\cdot + j_m} C(u_{1,j_1}, \\cdot\\cdot\\cdot, u_{m,j_m} | \\Gamma)\n\\] Where \\(u_{j,1}\\) = \\(F_j(x_j^-)\\) and \\(u_{j,2}\\) = \\(F_j(x_j)\\).\nNote that this process is markedly more computationally costly than the continuous case, particularly as the number of outcomes increases - as an additional dimension of integration is added for each outcome. Additionally, evaluating the density for copula with discrete marginals now requires the evaluation of the \\(m\\)-variate CDF, where the continuous case only required the evaluation of the \\(m\\)-variate density. While there are existing implementations for the multivariate Gaussian and Student-T CDFs, these require numerical integration and so are markedly more costly than the corresponding density functions.\nUsing the bivariate case as an example:\n\\[\nf(x_1, x_2) = \\sum_{j_1=1}^2 \\sum_{j_2=1}^2 (-1)^{j_1 + j_2} C(u_{1,j_1}, u_{2,j_2} | \\Gamma)\n\\]\n\\[\n= (-1)^{1+1} C(u_{1,1}, u_{2,1} | \\Gamma)\n\\]\n\\[\n+ (-1)^{1+2} C(u_{1,1}, u_{2,2} | \\Gamma)\n\\]\n\\[\n+ (-1)^{2+1} C(u_{1,2}, u_{2,1} | \\Gamma)\n\\]\n\\[\n+ (-1)^{2+2} C(u_{1,2}, u_{2,2} | \\Gamma)\n\\]\nWhich is more simply expressed as:\n\\[\nf(x_1, x_2) =\n\\] \\[\nC(F_1(x_1^-), F_2(x_2^-) | \\Gamma) \\\\\n\\]\n\\[\n  - C(F_1(x_1), F_2(x_2^-) | \\Gamma) \\\\\n\\]\n\\[\n  - C(F_1(x_1^-), F_2(x_2) | \\Gamma) \\\\\n\\]\n\\[\n  + C(F_1(x_1), F_2(x_2) | \\Gamma)\n\\]\nWhere \\(C(\\dot)\\) is the copula function. Note that in the univariate case, this simplifies to the marginal PMF:\n\\[\nf(x_1 | I_1) = \\Phi_1(\\Phi^{-1}(F_1(x_1)) | I_1) - \\Phi_1(\\Phi^{-1}(F_1(x_1^-)) | I_1) \\\\\n\\] \\[\n= \\Phi(\\Phi^{-1}(F_1(x_1))) - \\Phi(\\Phi^{-1}(F_1(x_1^-))) \\\\\n\\] \\[\n= F_1(x_1) - F_1(x_1^-)\n\\]\nAs \\(\\Phi_1(\\cdot | I_1) == \\Phi(\\cdot)\\) (i.e., a multivariate CDF with one outcome and unit variance is equivalent to the univariate CDF). \n\n\n\nClearly, directing estimating the likelihood by marginalising over the uniform variables is computationally expensive. A more efficient alternative was proposed by Smith & Khaled (2011), which was the use of data augmentation to treat the \\(u_m\\) uniform variables as parameters to be estimated. In other words, for each observation we estimate a uniform parameter bounded by the marginal CDF evaluated at the upper and lower bounds:\n\\[\nu_m \\sim U\\left(F_m(x_m^-), F_m(x_m)\\right)\n\\]\nIn Smith & Khaled’s (2011) original paper, this still required some computational complexity to implement, as they were using a Gibbs sampler - requiring the specification of conditional distributions. This is much simpler in Stan (and other HMC samplers), where we only need to specify the joint density function. This means that we simply specify the uniform parameters and use them in the copula density function that we defined above.\nTo illustrate, we will use a Gaussian copula to model the correlation \\(\\rho\\) between a Poisson-distributed outcome \\(y_1\\) with rate parameter \\(\\lambda\\) and a binomial outcome \\(y_2\\) with probability \\(\\theta\\).\nAs with the example above, we will first use the copula approach to generate the correlated outcomes:\n\nlambda_r &lt;- 10\ntheta_r &lt;- 0.7\nrho_r &lt;- 0.5\ny_denom &lt;- sample(1:100, 500, replace = TRUE)\n\nz_discrete &lt;- matrix(rnorm(1000), ncol = 2) %*%\n                chol(matrix(c(1, rho_r, rho_r, 1), nrow = 2))\ny_discrete &lt;- pnorm(z_discrete)\ny_discrete[,1] &lt;- qpois(y_discrete[,1], lambda_r)\ny_discrete[,2] &lt;- qbinom(y_discrete[,2], size = y_denom, prob = theta_r)\n\nNext, we update our Stan model with the data augmentation approach. As Stan allows us to specify parameter bounds directly, we simply need a function which evaluates the marginal CDFs at the upper and lower bounds for each observation and returns these as the bounds for the uniform parameters. This is implemented as the uvar_bounds function in the Stan model below.\nThen the u parameters are used directly in the copula density function:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(array[] int pois_y, array[] int binom_y,\n                     array[] int binom_N, real lambda, real theta,\n                     int is_upper) {\n    int N = size(pois_y);\n    matrix[N, 2] u_bounds;\n\n    for (n in 1:N) {\n      if (is_upper == 0) {\n        u_bounds[n, 1] = pois_y[n] == 0.0\n                          ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n        u_bounds[n, 2] = binom_y[n] == 0.0\n                          ? 0.0 : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[n, 1] = poisson_cdf(pois_y[n] | lambda);\n        u_bounds[n, 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int pois_y;\n  array[N] int binom_y;\n  array[N] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(pois_y, binom_y, binom_N, lambda, theta, 0),\n    upper=uvar_bounds(pois_y, binom_y, binom_N, lambda, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[2] rho_chol;\n}\n\nmodel {\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n}\n\nNext we follow the same process as the continuous case to fit the model:\n\ncopula_discrete_mod &lt;- cmdstan_model(\"gauss_copula_discrete.stan\")\n\nThen we can use the compiled model object to sample our parameters:\n\ncopula_discrete_fit &lt;- copula_discrete_mod$sample(\n                                data = list(N = nrow(y_discrete),\n                                            pois_y = y_discrete[,1],\n                                            binom_y = y_discrete[,2],\n                                            binom_N = y_denom),\n                                parallel_chains = 4)\n\nWe can see that we were able to successfully recover both the marginal and correlation data-generating parameters:\n\ncopula_discrete_fit$summary(variables = c(\"lambda\", \"theta\", \"rho\"))\n\n# A tibble: 3 × 10\n  variable   mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda   10.2   10.2   0.139   0.139   9.97  10.4    1.00    4351.    3464.\n2 theta     0.699  0.699 0.00292 0.00280 0.694  0.703  1.00    4831.    2970.\n3 rho       0.519  0.520 0.0304  0.0305  0.468  0.566  1.00    4375.    2938.\n\n\nAdditionally, the time taken to fit the model was quite reasonable for the sample size (500):\n\ncopula_discrete_fit$time()$total\n\n[1] 290.1294\n\n\n\n\n\n\nCombining both continuous and discrete marginals is a simple combination of the two approaches, where the Copula function \\(C(\\cdot)\\) now includes both the continuous and discrete components while marginalising over the \\(u\\) for the discrete components.\nTo illustrate, consider if we extended the bivariate Poisson example to include two continuous outcomes \\(x_3\\) & \\(x_4\\) with arbitrary density functions \\(f_3(\\cdot)\\) & \\(f_4(\\cdot)\\) and marginal CDFs \\(F_3(\\cdot)\\) & \\(F_4(\\cdot)\\).\nThe joint density function would now be given as:\n\\[\nf(x_1, x_2, x_3, x_4) = \\\\\n\\]\n\\[\nf_3(x_3) \\cdot f_4(x_4) \\cdot \\sum_{j_1=1}^2 \\sum_{j_2=1}^2 (-1)^{j_1 + j_2} C(u_{1,j_1}, u_{2,j_2}, F_3(x_3), F_4(x_4) | \\Gamma)\n\\]\nOr, given that we are using the data-augmentation approach for the discrete marginals:\n\\[\nu_1 \\sim U\\left(F_1(x_1^-), F_1(x_1)\\right)\n\\]\n\\[\nu_2 \\sim U\\left(F_2(x_2^-), F_2(x_2)\\right)\n\\]\n\\[\nf(x_1, x_2, x_3, x_4) = f_3(x_x) \\cdot f_4(x_4) \\cdot c(u_1, u_2, F_3(x_3), F_4(x_4) | \\Gamma)\n\\]\nClearly, this allows for the trivial modelling of any combination of continuous and discrete marginals.\nTo illustrate, we will combine the previous two approaches, and estimate a Gaussian copula with marginals: \\[\ny_1 \\sim \\text{Exponential}(\\lambda)\n\\]\n\\[\ny_2 \\sim \\text{Chi-Square}(\\nu)\n\\]\n\\[\ny_3 \\sim \\text{Poisson}(\\lambda_r)\n\\]\n\\[\ny_4 \\sim \\text{Binomial}(N, \\theta_r)\n\\]\nAs with the previous examples, we will first generate the correlated outcomes:\n\ngamma &lt;- randcorr::randcorr(4)\nz &lt;- matrix(rnorm(2000), ncol = 4) %*% chol(gamma)\ny_mix &lt;- pnorm(z)\ny_mix[,1] &lt;- qexp(y_mix[,1], rate = lambda)\ny_mix[,2] &lt;- qchisq(y_mix[,2], df = nu)\n\nymix_denom &lt;- sample(1:100, 500, replace = TRUE)\ny_mix[,3] &lt;- qpois(y_mix[,3], lambda_r)\ny_mix[,4] &lt;- qbinom(y_mix[,4], size = ymix_denom, prob = theta_r)\n\nAnd update our Stan model simply append the bounded uniform parameters (for the discrete marginals) to the evaluations of the marginal CDFs for the continuous marginals, before passing these to the copila density function:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(array[] int pois_y, array[] int binom_y,\n                     array[] int binom_N, real lambda, real theta,\n                     int is_upper) {\n    int N = size(pois_y);\n    matrix[N, 2] u_bounds;\n\n    for (n in 1:N) {\n      if (is_upper == 0) {\n        u_bounds[n, 1] = pois_y[n] == 0.0\n                          ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n        u_bounds[n, 2] = binom_y[n] == 0.0\n                          ? 0.0 : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[n, 1] = poisson_cdf(pois_y[n] | lambda);\n        u_bounds[n, 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n  array[N] int pois_y;\n  array[N] int binom_y;\n  array[N] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda_exp;\n  real&lt;lower=0&gt; nu;\n  real&lt;lower=0&gt; lambda_pois;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(pois_y, binom_y, binom_N, lambda_pois, theta, 0),\n    upper=uvar_bounds(pois_y, binom_y, binom_N, lambda_pois, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[4] rho_chol;\n}\n\nmodel {\n  matrix[N, 4] u_mix;\n  for (n in 1:N) {\n    u_mix[n, 1] = exponential_cdf(Y[n,1] | lambda_exp);\n    u_mix[n, 2] = chi_square_cdf(Y[n,2] | nu);\n    u_mix[n, 3] = u[n, 1];\n    u_mix[n, 4] = u[n, 2];\n  }\n\n  Y[, 1] ~ exponential(lambda_exp);\n  Y[, 2] ~ chi_square(nu);\n  u_mix ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  corr_matrix[4] rho = multiply_lower_tri_self_transpose(rho_chol);\n}\n\n\ncopula_mix_mod &lt;- cmdstan_model(\"copula_mix.stan\")\n\n\ncopula_mix_fit &lt;- copula_mix_mod$sample(\n                                data = list(N = nrow(y_mix),\n                                            Y = y_mix[,1:2],\n                                            pois_y = y_mix[,3],\n                                            binom_y = y_mix[,4],\n                                            binom_N = ymix_denom),\n                                parallel_chains = 4)\n\nWe can see that the data-generating parameters (both for the marginals and dependence between them) have all been successfully recovered\n\ncopula_mix_fit$summary(variables = c(\"lambda_exp\", \"nu\", \"lambda_pois\", \"theta\"))\n\n# A tibble: 4 × 10\n  variable     mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda_exp  2.05   2.05  0.0788  0.0799  1.93   2.18   1.00    8062.    2571.\n2 nu          4.97   4.97  0.0878  0.0891  4.83   5.12   1.00    4338.    3371.\n3 lambda_pois 9.94   9.94  0.140   0.142   9.71  10.2    1.00   10417.    3086.\n4 theta       0.701  0.701 0.00161 0.00158 0.698  0.703  1.00    4280.    3131.\n\n\n\ngamma\n\n            [,1]       [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.4562896  0.02669022 -0.69232431\n[2,] -0.45628959  1.0000000 -0.10901915  0.92526889\n[3,]  0.02669022 -0.1090192  1.00000000  0.06522568\n[4,] -0.69232431  0.9252689  0.06522568  1.00000000\n\ncopula_mix_fit$summary(variables = c(\"rho\"))\n\n# A tibble: 16 × 10\n   variable     mean   median      sd     mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 rho[1,1]  1        1       0       0        1        1      NA         NA \n 2 rho[2,1] -0.431   -0.431   0.0262  0.0266  -0.473   -0.387   1.00    4173.\n 3 rho[3,1]  0.0758   0.0760  0.0416  0.0421   0.00660  0.144   1.00    8283.\n 4 rho[4,1] -0.679   -0.679   0.0176  0.0181  -0.708   -0.650   1.00    4176.\n 5 rho[1,2] -0.431   -0.431   0.0262  0.0266  -0.473   -0.387   1.00    4173.\n 6 rho[2,2]  1        1       0       0        1        1      NA         NA \n 7 rho[3,2] -0.153   -0.153   0.0392  0.0389  -0.216   -0.0879  1.00    5701.\n 8 rho[4,2]  0.923    0.923   0.00466 0.00481  0.915    0.930   1.00    6795.\n 9 rho[1,3]  0.0758   0.0760  0.0416  0.0421   0.00660  0.144   1.00    8283.\n10 rho[2,3] -0.153   -0.153   0.0392  0.0389  -0.216   -0.0879  1.00    5701.\n11 rho[3,3]  1        1       0       0        1        1      NA         NA \n12 rho[4,3]  0.00589  0.00578 0.0403  0.0390  -0.0603   0.0718  1.00    5912.\n13 rho[1,4] -0.679   -0.679   0.0176  0.0181  -0.708   -0.650   1.00    4176.\n14 rho[2,4]  0.923    0.923   0.00466 0.00481  0.915    0.930   1.00    6795.\n15 rho[3,4]  0.00589  0.00578 0.0403  0.0390  -0.0603   0.0718  1.00    5912.\n16 rho[4,4]  1        1       0       0        1        1      NA         NA \n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\ncopula_mix_fit$time()$total\n\n[1] 498.0359\n\n\n\n\n\nThe use of data augmentation with discrete marginals also allows for the trivial handling of missing data on discrete outcomes. While missing continuous data in Stan models can easily by handled by estimating the observation as a parameter in the model, the same approach cannot be used for discrete data - as HMC-based methods like Stan cannot estimate discrete parameters. However, as the data-augmented copula model is already estimating a uniform parameter for each discrete observation, we can simply set the bounds for the uniform parameter to \\((0,1)\\) (as we do not have an observed CDF for the bounds).\nAs such, the uniform parameters are instead estimated as:\n\\[\nu_m \\sim \\begin{cases}\n  U\\left(F_m(x_m^-), F_m(x_m)\\right) & \\text{if } x_m \\text{ is observed} \\\\\n  U(0, 1) & \\text{if } x_m \\text{ is missing}\n\\end{cases}\n\\]\nThis is also trivial to integrate into our existing Stan model. We simply update our bounds function (uvar_bounds()) to set the bounds to \\((0,1)\\) by default, and then set the bounds to \\((F_m(x_m^-), F_m(x_m))\\) for the observed outcomes:\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  matrix uvar_bounds(int N, array[] int pois_y, array[] int binom_y,\n                      array[] int binom_N, array[] int pois_y_ind,\n                      array[] int binom_y_ind, real lambda, real theta,\n                      int is_upper) {\n    matrix[N, 2] u_bounds = rep_matrix(is_upper, N, 2);\n\n    for (n in 1:size(pois_y_ind)) {\n      if (is_upper == 0) {\n        u_bounds[pois_y_ind[n], 1] = pois_y[n] == 0.0\n                                      ? 0.0 : poisson_cdf(pois_y[n] - 1 | lambda);\n      } else {\n        u_bounds[pois_y_ind[n], 1] = poisson_cdf(pois_y[n] | lambda);\n      }\n    }\n\n    for (n in 1:size(binom_y_ind)) {\n      if (is_upper == 0) {\n        u_bounds[binom_y_ind[n], 2] = binom_y[n] == 0.0 ? 0.0\n                                      : binomial_cdf(binom_y[n] - 1 | binom_N[n], theta);\n      } else {\n        u_bounds[binom_y_ind[n], 2] = binomial_cdf(binom_y[n] | binom_N[n], theta);\n      }\n    }\n\n    return u_bounds;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; Npois;\n  int&lt;lower=0&gt; Nbinom;\n  matrix[N, 2] Y;\n  array[Npois] int pois_y;\n  array[Npois] int pois_y_ind;\n  array[Nbinom] int binom_y;\n  array[Nbinom] int binom_y_ind;\n  array[Nbinom] int binom_N;\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda_exp;\n  real&lt;lower=0&gt; nu;\n  real&lt;lower=0&gt; lambda_pois;\n  real&lt;lower=0, upper=1&gt; theta;\n  matrix&lt;\n    lower=uvar_bounds(N, pois_y, binom_y, binom_N, pois_y_ind,\n                      binom_y_ind, lambda_pois, theta, 0),\n    upper=uvar_bounds(N, pois_y, binom_y, binom_N, pois_y_ind,\n                      binom_y_ind, lambda_pois, theta, 1)\n  &gt;[N, 2] u;\n  cholesky_factor_corr[4] rho_chol;\n}\n\nmodel {\n  matrix[N, 4] u_mix;\n  for (n in 1:N) {\n    u_mix[n, 1] = exponential_cdf(Y[n,1] | lambda_exp);\n    u_mix[n, 2] = chi_square_cdf(Y[n,2] | nu);\n    u_mix[n, 3] = u[n, 1];\n    u_mix[n, 4] = u[n, 2];\n  }\n\n  Y[, 1] ~ exponential(lambda_exp);\n  Y[, 2] ~ chi_square(nu);\n\n  u_mix ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  corr_matrix[4] rho = multiply_lower_tri_self_transpose(rho_chol);\n}\n\nTo test this, we will randomly remove 10% of the data for each of the Poisson and Binomial outcomes and fit the model:\n\ny_mix_miss &lt;- y_mix\ny_mix_miss[sample(1:500, 50), 3] &lt;- NA\ny_mix_miss[sample(1:500, 50), 4] &lt;- NA\npois_y_ind &lt;- which(!is.na(y_mix_miss[, 3]))\nbinom_y_ind &lt;- which(!is.na(y_mix_miss[, 4]))\n\nstandata &lt;-  list(N = nrow(y_mix_miss),\n                        Npois = length(pois_y_ind),\n                        Nbinom = length(binom_y_ind),\n                        Y = y_mix_miss[, 1:2],\n                        pois_y = y_mix_miss[pois_y_ind, 3],\n                        pois_y_ind = pois_y_ind,\n                        binom_y = y_mix_miss[binom_y_ind, 4],\n                        binom_y_ind = binom_y_ind,\n                        binom_N = ymix_denom[binom_y_ind])\n\n\ncopula_mix_missing_mod &lt;- cmdstan_model(\"copula_mix_missing.stan\")\n\n\ncopula_mix_missing_fit &lt;- copula_mix_missing_mod$sample(data = standata, parallel_chains = 4)\n\nWe can see that even with missing discrete data, the data-generating parameters have been successfully recovered:\n\ncopula_mix_missing_fit$summary(variables = c(\"lambda_exp\", \"nu\", \"lambda_pois\", \"theta\"))\n\n# A tibble: 4 × 10\n  variable      mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lambda_exp   2.06   2.06  0.0810  0.0806  1.93   2.20   1.00    6094.    2952.\n2 nu           4.98   4.98  0.0908  0.0882  4.83   5.14   1.00    2905.    2946.\n3 lambda_pois 10.0   10.0   0.148   0.151   9.79  10.3    1.00   10154.    2937.\n4 theta        0.701  0.701 0.00170 0.00166 0.698  0.704  1.00    2827.    3106.\n\n\n\ngamma\n\n            [,1]       [,2]        [,3]        [,4]\n[1,]  1.00000000 -0.4562896  0.02669022 -0.69232431\n[2,] -0.45628959  1.0000000 -0.10901915  0.92526889\n[3,]  0.02669022 -0.1090192  1.00000000  0.06522568\n[4,] -0.69232431  0.9252689  0.06522568  1.00000000\n\ncopula_mix_missing_fit$summary(variables = c(\"rho\"))\n\n# A tibble: 16 × 10\n   variable     mean   median      sd     mad      q5     q95   rhat ess_bulk\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 rho[1,1]  1        1       0       0        1       1      NA          NA \n 2 rho[2,1] -0.432   -0.432   0.0260  0.0261  -0.474  -0.390   1.00     5714.\n 3 rho[3,1]  0.0854   0.0861  0.0419  0.0420   0.0150  0.154   1.000    8959.\n 4 rho[4,1] -0.679   -0.679   0.0176  0.0174  -0.707  -0.649   1.00     5144.\n 5 rho[1,2] -0.432   -0.432   0.0260  0.0261  -0.474  -0.390   1.00     5714.\n 6 rho[2,2]  1        1       0       0        1       1      NA          NA \n 7 rho[3,2] -0.164   -0.165   0.0406  0.0414  -0.230  -0.0972  1.00     5941.\n 8 rho[4,2]  0.923    0.924   0.00488 0.00491  0.915   0.931   1.00     5662.\n 9 rho[1,3]  0.0854   0.0861  0.0419  0.0420   0.0150  0.154   1.000    8959.\n10 rho[2,3] -0.164   -0.165   0.0406  0.0414  -0.230  -0.0972  1.00     5941.\n11 rho[3,3]  1        1       0       0        1       1      NA          NA \n12 rho[4,3] -0.00876 -0.00858 0.0416  0.0425  -0.0767  0.0589  1.00     5934.\n13 rho[1,4] -0.679   -0.679   0.0176  0.0174  -0.707  -0.649   1.00     5144.\n14 rho[2,4]  0.923    0.924   0.00488 0.00491  0.915   0.931   1.00     5662.\n15 rho[3,4] -0.00876 -0.00858 0.0416  0.0425  -0.0767  0.0589  1.00     5934.\n16 rho[4,4]  1        1       0       0        1       1      NA          NA \n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\n\nLeave-one-out cross-validation (LOO-CV) is a popular and powerful tool for model comparison and selection. However, it can be difficult to compute for likelihoods which cannot be factorised into a product of individual likelihoods. Thankfully, Buerkner, Gabry, and Vehtari (2020) have previously derived a method for calculating approximate LOO-CV for non-factorisable models with a multivariate-normal outcome distribution.\n\n\nGiven a vector \\(y\\) which follows a multivariate-normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\):\n\\[\ny \\sim \\mathcal{MVN}(\\mu, \\Sigma)\n\\]\nThe pointwise likelihood is given by:\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(y_i | y_i - \\frac{g_i}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}})\n\\]\nWhere:\n\\[\ng_i = [\\Sigma^{-1}(y - \\mu)]_i\n\\]\n\\[\n\\bar{\\sigma}_{ii} = [\\Sigma^{-1}]_{ii}\n\\]\n\n\n\nAs demonstrated earlier, the Gaussian copula density can be expressed as a multivariate normal density divided by the product of the univariate standard normal densities. As such, we can calculate the approximate LOO-CV for a Gaussian copula by applying Buerkner, Gabry, and Vehtari (2020)’s method to the multivariate normal density to obtain a pointwise likelihood, and then divide by the respective univariate standard normal density.\nTo summarise, given that a Gaussian copula with continuous marginals has density function:\n\\[\nf(y) = c(\\Phi(y) | \\Gamma) \\prod_{i=1}^{m} f_i(y_i)\n\\]\n\\[\nc(\\Phi(y) | \\Gamma) = \\textrm{MVN}(q | 0, \\Gamma) \\cdot \\prod_{i=1}^m\\textrm{N}(q_i | 0, 1)^{-1}\n\\]\nThe pointwise conditional likelihood is given by:\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(q_i | q_i - \\frac{g_i}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}}) \\cdot \\textrm{N}(q_i | 0, 1)^{-1} \\cdot f_i(y_i)\n\\]\nWe can then use the PSIS algorithm implemented in the loo package to approximate \\(p(y_i | y_{-i})\\).\n\n\n\nIn order to validate the approximation, we can also use the method proposed by Buerkner at. al (2020) to perform exact LOO-CV. To do so, the held-out observation \\(y_i\\) is estimated as a parameter in the model \\(y_i^{miss}\\) and substituted into the set of observations \\(y\\) before calculating the marginal CDFs:\n\\[\nu_{miss(i)} = \\left(F_1(y_1),...,F_{i-1}(y_{i-1}),F_i(y_i^{miss}),F_{i+1}(y_{i+1}),..., F_m(y_m)\\right)\n\\] Such that \\(q_{miss(i)}\\) is then the result of the standard normal quantile function for each element of \\(u_{miss(i)}\\). This new \\(q_{miss(i)}\\) set is then used to define the parameters for the log-predictive density when evaluating it the held-out observation \\(q_i\\):\n\\[\np(y_i | y_{-i}, \\theta) = \\mathcal{N}(q_i | q_i^{miss} - \\frac{g_i^{miss}}{\\bar{\\sigma}_{ii}}, \\sqrt{\\bar{\\sigma}_{ii}^{-1}}) \\cdot \\textrm{N}(q_i | 0, 1)^{-1} \\cdot f_i(y_i)\n\\] Where: \\[\ng_i^{miss} = [\\Gamma^{-1}(q_{miss(i)})]_i\n\\] And we can then estimate the leave-one-out density for the held-out observation using the posterior samples:\n\\[\np(y_i | y_{-i}) = \\sum_{s=1}^{S} p(y_i | y_{-i}, \\theta_{-i}^{(s)})\n\\]\n\n\n\nTo implement these, we will update our Stan model to either calculate the exact LOO density for a held-out observation (and estimate that observation as a parameter), or to calculate the density for all observations for use in approximate LOO-CV estimation.\nWe add a data argument y_miss_i to the model to specify the index of the missing observation. If y_miss_i is 0, the model will calculate the approximate LOO-CV density for all observations. If y_miss_i is greater than 0, the model will calculate the exact LOO-CV density for the observation at index y_miss_i.\n\nfunctions {\n  real gauss_copula_cholesky_lpdf(matrix u, matrix L) {\n    array[rows(u)] row_vector[cols(u)] q;\n    for (n in 1:rows(u)) {\n      q[n] = inv_Phi(u[n]);\n    }\n\n    return multi_normal_cholesky_lpdf(q | rep_row_vector(0, cols(L)), L)\n            - std_normal_lpdf(to_vector(to_matrix(q)));\n  }\n\n  row_vector gauss_copula_cholesky_pointwise(row_vector y_obs, row_vector y_miss,\n                                              real lambda, real nu, matrix Sigma_inv) {\n    int J = cols(Sigma_inv);\n    vector[J] inv_sigma_inv = inv(diagonal(Sigma_inv));\n\n    row_vector[J] log_lik;\n    vector[2] u_miss = [ exponential_cdf(y_miss[1] | lambda),\n                          chi_square_cdf(y_miss[2] | nu) ]';\n    vector[2] u = [ exponential_cdf(y_obs[1] | lambda),\n                    chi_square_cdf(y_obs[2] | nu) ]';\n    vector[2] q_miss = inv_Phi(u_miss);\n    vector[2] q = inv_Phi(u);\n    vector[2] g = Sigma_inv * q_miss;\n\n    log_lik = [ exponential_lpdf(y_obs[1] | lambda),\n                      chi_square_lpdf(y_obs[2] | nu) ];\n\n    for (j in 1:J) {\n      log_lik[j] += normal_lpdf(q[j] | q_miss[j] - g[j] * inv_sigma_inv[j],\n                                        sqrt(inv_sigma_inv[j]))\n                      - std_normal_lpdf(q[j]);\n    }\n    return log_lik;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 2] Y;\n  int y_miss_i;\n}\n\ntransformed data {\n  int has_missing = y_miss_i &gt; 0;\n  int y_miss_col = (y_miss_i - 1) %/% N + 1;\n  int y_miss_row = y_miss_i - (y_miss_col - 1) * N;\n  array[2] int y_miss_idx = { y_miss_row, y_miss_col };\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda;\n  real&lt;lower=0&gt; nu;\n  cholesky_factor_corr[2] rho_chol;\n  array[has_missing] real&lt;lower=0&gt; y_miss;\n}\n\nmodel {\n  matrix[N, 2] u;\n  matrix[N, 2] Ymiss = Y;\n  if (has_missing) {\n    Ymiss[y_miss_idx[1], y_miss_idx[2]] = y_miss[1];\n  }\n\n  for (n in 1:N) {\n    u[n, 1] = exponential_cdf(Ymiss[n, 1] | lambda);\n    u[n, 2] = chi_square_cdf(Ymiss[n, 2] | nu);\n  }\n\n  Ymiss[, 1] ~ exponential(lambda);\n  Ymiss[, 2] ~ chi_square(nu);\n\n  u ~ gauss_copula_cholesky(rho_chol);\n}\n\ngenerated quantities {\n  real rho = multiply_lower_tri_self_transpose(rho_chol)[1, 2];\n  vector[has_missing ? 1 : N*2] log_lik;\n\n  {\n    int J = 2;\n    matrix[J,J] Sigma_inv = chol2inv(rho_chol);\n    matrix[has_missing ? 1 : N, J] log_lik_mat;\n    array[has_missing ? 1 : N] int iters = has_missing ? {y_miss_idx[1]}\n                                            : linspaced_int_array(N, 1, N);\n\n    for (n in iters) {\n      row_vector[J] Ymiss = Y[n];\n      if (has_missing) {\n        Ymiss[y_miss_idx[2]] = y_miss[1];\n      }\n      log_lik_mat[has_missing ? 1 : n]\n          = gauss_copula_cholesky_pointwise(Y[n], Ymiss, lambda, nu, Sigma_inv);\n    }\n\n    if (has_missing) {\n      log_lik[1] = log_lik_mat[1, y_miss_idx[2]];\n    } else {\n      log_lik = to_vector(log_lik_mat);\n    }\n  }\n}\n\n\ncopula_mod_loo &lt;- cmdstan_model(\"copula_mod_loo.stan\")\n\nFirst we’ll estimate the approximate LOO-CV for all observations:\n\ncopula_loo_fit &lt;- copula_mod_loo$sample(data = list(N = nrow(y), Y = y, y_miss_i = 0),\n                                parallel_chains = 4)\napprox_loo &lt;- copula_loo_fit$loo()\n\nNext, we’ll estimate the exact LOO-CV for each observation. Given the number of observations to hold-out (1000), we will do this parallel using the furrr and future packages:\n\nfuture::plan(future::multisession)\n\nrun_fun &lt;- function(idx) {\n  copula_mod &lt;- cmdstanr::cmdstan_model(exe_file = copula_mod_loo$exe_file())\n  copula_fit_iter &lt;- copula_mod$sample(\n    data = list(N = nrow(y), Y = y, y_miss_i = idx)\n  )\n\n  log_lik_draws &lt;- copula_fit_iter$draws(variables = \"log_lik\", format = \"draws_df\")$`log_lik[1]`\n\n  data.frame(res = as.numeric(log_lik_draws)) |&gt;\n    setNames(paste0(\"log_lik_\", idx))\n}\n\nexact_lpd &lt;- furrr::future_map_dfc(1:1000, run_fun, .progress = TRUE)\nlog_mean_exp &lt;- function(x) {\n  # more stable than log(mean(exp(x)))\n  max_x &lt;- max(x)\n  max_x + log(sum(exp(x - max_x))) - log(length(x))\n}\nexact_elpds &lt;- apply(exact_lpd, 2, log_mean_exp)\n\nfuture::plan(future::sequential)\n\nThen we can visually compare the approximate and exact LOO-CV estimates:\n\nlibrary(ggplot2)\nsuppressPackageStartupMessages(library(bayesplot))\ncolor_scheme_set(\"brightblue\")\ntheme_set(theme_default())\n\n# Create a summary annotation for the plot\nplot_lab &lt;- paste0(paste0(\"Exact ELPD: \", round(sum(exact_elpds), 2)), \"\\n\",\n              paste0(\"Approx ELPD: \", round(sum(approx_loo$pointwise[,1]), 2)))\n\ndata.frame(loo_elpd = approx_loo$pointwise[,1],\n           exact_lpd = exact_elpds) |&gt;\n  ggplot(aes(x = loo_elpd, y = exact_lpd)) +\n    geom_abline(color = \"gray30\") +\n    geom_point(size = 2) +\n    xlab(\"Approximate elpds\") +\n    ylab(\"Exact elpds\") +\n    annotate(\"text\", x = 0, y = -6, label = plot_lab)\n\n\n\n\n\n\n\n\nWe can see that the exact and approximate LOO-CV estimates are extremely consistent.",
    "crumbs": [
      "Case Studies",
      "Copulas"
    ]
  },
  {
    "objectID": "R_C_Math.html",
    "href": "R_C_Math.html",
    "title": "Using R’s C Functions in Stan Models",
    "section": "",
    "text": "The R programming language provides robust, precise, and efficient implementations of many mathematical functions and distributions. While these can be used in Stan models by interfacing with an R session, this involves significant overhead and can result in slow sampling.\nGiven that a majority of R’s core functions and distributions are implemented in C, we can use Stan’s external C++ framework to call them directly, avoiding the need and overhead of an R session entirely.",
    "crumbs": [
      "Case Studies",
      "Using R's C Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_C_Math.html#motivation",
    "href": "R_C_Math.html#motivation",
    "title": "Using R’s C Functions in Stan Models",
    "section": "",
    "text": "The R programming language provides robust, precise, and efficient implementations of many mathematical functions and distributions. While these can be used in Stan models by interfacing with an R session, this involves significant overhead and can result in slow sampling.\nGiven that a majority of R’s core functions and distributions are implemented in C, we can use Stan’s external C++ framework to call them directly, avoiding the need and overhead of an R session entirely.",
    "crumbs": [
      "Case Studies",
      "Using R's C Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_C_Math.html#r-function-quantile-functions---log-inputs",
    "href": "R_C_Math.html#r-function-quantile-functions---log-inputs",
    "title": "Using R’s C Functions in Stan Models",
    "section": "R Function: Quantile Functions - log inputs",
    "text": "R Function: Quantile Functions - log inputs\nWhen evaluating quantile functions we often prefer to provide the input probability on the log-scale to avoid numerical issues with under- or overflow. For the present example, we will use R’s qnorm() and qt() functions to calculate the quantile function for a standard-normal distribution and a standard student-t distribution where the input probability is provided on the log scale:\nThe R signature for the qnorm() function is:\n\nqnorm(p, mean, sd, lower.tail, log.p)\n\nWhich maps directly to the underlying C implementation:\n\ndouble  qnorm(double, double, double, int, int);\n\n\nGradients\nWe will also need to calculate gradients for inputs if we are aiming to use the function with parameters in a Stan model. The gradients for quantile function, with respect to the probability parameter, are given by the reciprocal of the density, and the adjustment for inputs on the log-scale are easily given by the chain rule:\n\\[\n\\frac{\\text{d}}{\\text{d}p}F^{-1}(e^p) \\\\\n= \\frac{\\text{d}}{\\text{d}x}F^{-1}(x) \\cdot \\frac{\\text{d}}{\\text{d}p}e^p \\\\\n= \\frac{e^p}{f(F^{-1}(e^p))}\n\\]\nThis means that we can also use R’s corresponding density functions, dnorm() and dt(), to define the gradients\n\n\nStan - External C++\nTo use the density and quantile functions in our external C++, we simply include R’s math header and then define our functions as usual:\n\n#include &lt;stan/math.hpp&gt;\n#include &lt;Rmath.h&gt;\n\ndouble qnorm_logp(double p, std::ostream* pstream__) {\n  return qnorm(p, 0, 1, 1, 1);\n}\n\ndouble qt_logp(double p, double df, std::ostream* pstream__) {\n  return qt(p, df, 1, 1);\n}\n\nstan::math::var qt_logp(stan::math::var p, double df, std::ostream* pstream__) {\n  return stan::math::make_callback_var(\n    qt_logp(p.val(), df, pstream__),\n    [p, df](auto& vi) mutable {\n      // Calculate gradient on log-scale for numerical stability\n      p.adj() += vi.adj() * exp(p.val() - dt(vi.val(), df, 1));\n    }\n  );\n}\n\nstan::math::var qnorm_logp(stan::math::var p, std::ostream* pstream__) {\n  return stan::math::make_callback_var(\n    qnorm_logp(p.val(), pstream__),\n    [p](auto& vi) mutable {\n      // Calculate gradient on log-scale for numerical stability\n      p.adj() += vi.adj() * exp(p.val() - dnorm(vi.val(), 0, 1, 1));\n    }\n  );\n}\n\n\n\nStan - Stan Model\nWe will use the following (nonsensical) Stan model to test the values and gradients of the implementation:\n\nfunctions {\n  real qnorm_logp(real logp);\n  real qt_logp(real logp, data real df);\n}\n\ndata {\n  int use_normal;\n}\n\nparameters {\n  real&lt;upper=0&gt; log_p;\n}\n\ntransformed parameters {\n  real qnorm_test = qnorm_logp(log_p);\n  real qt_test = qt_logp(log_p, 3);\n}\n\nmodel {\n  target += use_normal ? qnorm_test : qt_test;\n}\n\n\n\nStan - Compilation & Linking\nIn order for the Stan model to be able to use the C functions from R, we need to provide additional flags to the compilation & linking of the model. Thankfully, R has built-in functions which return these flags:\n\n# R library stored in different directory on windows\nlibdir &lt;- ifelse(.Platform$OS.type == \"windows\", R.home(\"bin\"), R.home(\"lib\"))\n\ncpp_options = list(\n  paste0(\"CPPFLAGS += -I\", shQuote(R.home(\"include\"))),\n  paste0(\"LDLIBS += -L\", shQuote(libdir), \" -lR\")\n)\n\nWe can then pass these directly to cmdstanr, along with our model and external C++, for compilation:\n\nmod &lt;- cmdstanr::cmdstan_model(stan_file,\n                               user_header = user_hpp,\n                               cpp_options = cpp_options,\n                               force_recompile = TRUE)\n\n\n\nStan - Validation\nTo test our implementation, we can fit the model for a small number of iterations and check that the calculated quantiles are consistent with those returned by qnorm() in R directly, which shows that the values match completely.\n\nfit &lt;- mod$sample(data = list(use_normal = 0), chains = 1,\n                      iter_warmup = 50, iter_sampling = 50,\n                      show_messages = FALSE,\n                      show_exceptions = FALSE)\n\nWarning: 12 of 50 (24.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.3.\nSee https://mc-stan.org/misc/warnings for details.\n\nfit$draws(variables = c(\"log_p\", \"qnorm_test\", \"qt_test\")) |&gt;\n    posterior::mutate_variables(qnorm_true = qnorm(log_p, log.p=TRUE),\n                                qt_true = qt(log_p, df = 3, log.p=TRUE)) |&gt;\n    posterior::summarise_draws()\n\n# A tibble: 5 × 10\n  variable       mean   median       sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 log_p      -7.78e-8 -4.76e-8  6.01e-8 1.34e-8 -2.17e-7 -3.85e-8  2.79     1.42\n2 qnorm_test  5.28e+0  5.34e+0  1.10e-1 5.18e-2  5.05e+0  5.37e+0  2.79     1.42\n3 qt_test     2.64e+2  2.87e+2  4.79e+1 2.73e+1  1.72e+2  3.06e+2  2.79     1.42\n4 qnorm_true  5.28e+0  5.34e+0  1.10e-1 5.18e-2  5.05e+0  5.37e+0  2.79     1.42\n5 qt_true     2.64e+2  2.87e+2  4.79e+1 2.73e+1  1.72e+2  3.06e+2  2.79     1.42\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nNext, to validate the specification of gradients, we can use cmdstanr’s $diagnose() method to check that our gradient calculations match those from finite-differencing, which also shows a great match:\n\nmod$diagnose(data = list(use_normal = 1))$gradients()\n\n  param_idx  value     model finite_diff       error\n1         0 1.2461 -0.540771   -0.540771 4.72193e-11\n\nmod$diagnose(data = list(use_normal = 0))$gradients()\n\n  param_idx    value    model finite_diff       error\n1         0 -1.37989 0.159573    0.159573 1.20318e-10",
    "crumbs": [
      "Case Studies",
      "Using R's C Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html",
    "href": "R_In_Stan.html",
    "title": "Using R Functions in Stan Models",
    "section": "",
    "text": "A common question/request is for the ability to use R functions and packages as part of a Stan model specification. This requires two components:\n\nCalling R functions from C++\nGradients with respect to each function input\n\nWhile Stan can automatically calculate the gradients for C++ functions if they are implemented using existing functions with gradients, this cannot be extended to R functions. This means that any usage of R-based functions with parameters (i.e., not in the transformed data or generated quantities blocks) requires that a function for calculating the gradients is also implemented.\nThis document will provide a worked example for the process of using an R function in a Stan model via external C++.\nWe will be implementing the log determinant function:\n\\[\n\\log |M|\n\\]\nWhich has gradients:\n\\[\n\\frac{d}{dM} \\log |M| = \\left(M^{-1}\\right)^T\n\\]\nNote that the log determinant is already available in Stan as the log_determinant() function, which we will use to verify that the implementation is correct.",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html#introduction",
    "href": "R_In_Stan.html#introduction",
    "title": "Using R Functions in Stan Models",
    "section": "",
    "text": "A common question/request is for the ability to use R functions and packages as part of a Stan model specification. This requires two components:\n\nCalling R functions from C++\nGradients with respect to each function input\n\nWhile Stan can automatically calculate the gradients for C++ functions if they are implemented using existing functions with gradients, this cannot be extended to R functions. This means that any usage of R-based functions with parameters (i.e., not in the transformed data or generated quantities blocks) requires that a function for calculating the gradients is also implemented.\nThis document will provide a worked example for the process of using an R function in a Stan model via external C++.\nWe will be implementing the log determinant function:\n\\[\n\\log |M|\n\\]\nWhich has gradients:\n\\[\n\\frac{d}{dM} \\log |M| = \\left(M^{-1}\\right)^T\n\\]\nNote that the log determinant is already available in Stan as the log_determinant() function, which we will use to verify that the implementation is correct.",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html#rinside-calling-r-from-c",
    "href": "R_In_Stan.html#rinside-calling-r-from-c",
    "title": "Using R Functions in Stan Models",
    "section": "RInside: Calling R from C++",
    "text": "RInside: Calling R from C++\nThe process of interacting with R from C++ is greatly simplified by the RInside R package, which provides the C++ headers for initialising and managing an R session. The data structures that RInside uses for passing results between R and C++ are also designed for use with Rcpp and RcppEigen, which are needed for easy handling of matrix and vector types.\nHowever, a key detail that needs to be emphasised is that an R session can only be initialised once for a given program. This means that a single R session and its environment will be used for the entirety of the sampling/estimation process. Consequently, it should be considered best-practice to delete any variables/objects in the R session once they are no longer needed - otherwise you might be re-using objects/values from a previous iteration without realising.",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html#c",
    "href": "R_In_Stan.html#c",
    "title": "Using R Functions in Stan Models",
    "section": "C++:",
    "text": "C++:\nNext, we need to specify the C++ for our function to use with both arithmetic and autodiff (i.e., parameters) types:\n\n/**\n * The Stan headers *must* be included before the Rcpp headers, otherwise\n * internal definitions for Eigen are ignored\n*/\n#include &lt;stan/math.hpp&gt;\n#include &lt;RInside.h&gt;\n#include &lt;RcppEigen.h&gt;\n\n// If Stan detects R headers/environment it can cause issues for compilation\n// as it assumes the code is to be used by rstan\n#ifdef USING_R\n#undef USING_R\n#endif\n\n/**\n * The RInstance object is declared 'static' so that C++ knows not to delete\n * the object and invalidate the R session until the program has finished,\n * it also ensures that the created session is specific to this program\n * to avoid accessing other sessions (or having this session accessed by other processes)\n*/\nstatic RInside RInstance;\n\ntemplate &lt;typename T, stan::require_st_arithmetic&lt;T&gt;* = nullptr&gt;\ndouble r_log_determinant(const T& m, std::ostream *pstream__) {\n  /**\n   * Passing objects to R is very simple, as the conversion from C++ -&gt; R types\n   * is automatically delegated to Rcpp or similar (e.g., RcppEigen).\n  */\n  RInstance[\"m_val\"] = m;\n\n  /**\n   * Interacting with the R session is primarily through string commands.\n   *   - The `parseEval()` method is for commands which will return an object\n   *   - The `parseEvalQ()` method is for commands with no return value/object\n   *\n   * Rcpp again handles the process of converting the returned R object to the\n   * desired C++ type. You can allow this to be handled automatically, or you\n   * can wrap the call in `Rcpp::as&lt;T&gt;()` where `T` is the desired C++ type.\n  */\n  double log_det_val = RInstance.parseEval(\"determinant(m_val, logarithm = TRUE)$modulus\");\n\n  /**\n   * Make sure to clean-up the R environment before returning!\n  */\n  RInstance.parseEvalQ(\"rm(m_val)\");\n\n  return log_det_val;\n}\n\ntemplate &lt;typename T, stan::require_st_var&lt;T&gt;* = nullptr&gt;\nstan::math::var r_log_determinant(const T& m,\n                                  std::ostream *pstream__) {\n  using stan::arena_t;\n  using stan::math::var;\n  using stan::math::value_of;\n\n  /**\n   * The parameters are moved into Stans memory arena so that their gradients\n   * can be updated in the reverse pass\n  */\n  arena_t&lt;Eigen::Matrix&lt;var, -1, -1&gt;&gt; arena_m = m;\n\n  /**\n   * The R process is exactly the same as the non-parameter version, just with\n   * the addition of calculating the gradients for the input 'm'\n  */\n  RInstance[\"m_val\"] = stan::math::value_of(arena_m);\n  double log_det_val = RInstance.parseEval(\"determinant(m_val, logarithm = TRUE)$modulus\");\n  Eigen::MatrixXd log_det_grad = RInstance.parseEval(\"t(solve(m_val))\");\n  RInstance.parseEvalQ(\"rm(m_val)\");\n\n  /**\n   * Also move the calculated gradients into the memory arena so we can access\n   * them in the reverse pass\n  */\n  arena_t&lt;Eigen::MatrixXd&gt; log_det_grad_arena = log_det_grad;\n\n  /**\n   * Initialise a new parameter with the calculated value, and specify how the\n   * gradients for the inputs should be updated in the reverse pass\n  */\n  var log_det = log_det_val;\n  stan::math::reverse_pass_callback([arena_m, log_det, log_det_grad_arena]() mutable {\n    arena_m.adj() += log_det.adj() * log_det_grad_arena;\n  });\n\n  return log_det;\n}",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html#stan",
    "href": "R_In_Stan.html#stan",
    "title": "Using R Functions in Stan Models",
    "section": "Stan",
    "text": "Stan\n\nModel\nTo test that the R-implemented log_determinant() returns the same values and gradients as the built-in Stan function, we will use the following (nonsensical) model:\n\nfunctions {\n  real r_log_determinant(matrix x);\n}\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n  int use_r;\n}\nparameters {\n  matrix[2, 2] matrix_par;\n}\ntransformed parameters {\n  real log_det;\n  log_det = use_r ? r_log_determinant(matrix_par) : log_determinant(matrix_par);\n}\nmodel {\n  y ~ bernoulli_logit(log_det);\n}\n\n\n\nCompilation\nTo use this external C++ with Stan, we need to provide the additional compilation & linker flags for compiling against R, RInside, Rcpp, and RcppEigen. Both R and these packages provide methods for extracting these flags:\n\nextra_cxxflags &lt;- c(\n  paste0(\"-I\", shQuote(R.home(\"include\"))),\n  Rcpp:::RcppCxxFlags(),\n  RcppEigen:::RcppEigenCxxFlags(),\n  RInside:::RInsideCxxFlags()\n)\n\n# R library stored in different directory on windows\nlibdir &lt;- ifelse(.Platform$OS.type == \"windows\", R.home(\"bin\"), R.home(\"lib\"))\n\nextra_ldlibs &lt;- c(\n  paste0(\"-L\", shQuote(libdir), \" -lR\"),\n  RInside:::RInsideLdFlags()\n)\n\ncpp_options &lt;- list(\n  paste0(c(\"CXXFLAGS +=\", extra_cxxflags), collapse = \" \"),\n  paste0(c(\"LDLIBS +=\", extra_ldlibs), collapse = \" \")\n)\n\nThese will then be used to compile the model and external functions:\n\nmod &lt;- cmdstanr::cmdstan_model(stan_file,\n                               user_header = user_hpp,\n                               cpp_options = cpp_options,\n                               force_recompile = TRUE)\n\n\n\nEvaluation\nTo check our implementation, we’ll use the $diagnose() method to calculate the initial values and gradients for a model, and compare the results between the built-in log_determinant() and our implementation.\n\ndata &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1), use_r = 0)\n\nThe built-in returns:\n\nmod$diagnose(data = data, seed = 2023)$gradients()\n\n  param_idx     value    model finite_diff        error\n1         0  1.240610  1.06710     1.06710  4.57541e-10\n2         1  1.301600 -4.51515    -4.51515 -1.25091e-09\n3         2 -1.885300  3.11724     3.11724 -4.61351e-10\n4         3 -0.445567 -2.97116    -2.97116  1.70452e-10\n\n\nAnd our implementation:\n\ndata$use_r &lt;- 1\nmod$diagnose(data = data, seed = 2023)$gradients()\n\n  param_idx     value    model finite_diff        error\n1         0  1.240610  1.06710     1.06710  4.57541e-10\n2         1  1.301600 -4.51515    -4.51515 -1.25091e-09\n3         2 -1.885300  3.11724     3.11724 -4.61350e-10\n4         3 -0.445567 -2.97116    -2.97116  1.70450e-10\n\n\nAll looks good!",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  },
  {
    "objectID": "R_In_Stan.html#using-r-functions-without-analytic-gradients",
    "href": "R_In_Stan.html#using-r-functions-without-analytic-gradients",
    "title": "Using R Functions in Stan Models",
    "section": "Using R Functions without Analytic Gradients",
    "text": "Using R Functions without Analytic Gradients\nIn some cases, the analytic gradients for a given function might not always be known or easy to calculate. As a less-efficient alternative the gradients for a given function could also be computed numerically in R. For example, we could have calculated the gradients for our function using the numDeriv::grad() function:\n\nRInstance.parseEvalQ(\"detfun &lt;- function(x) { determinant(x, logarithm = TRUE)$modulus }\");\nEigen::MatrixXd log_det_grad = RInstance.parseEval(\"matrix(numDeriv::grad(detfun, m_val), nrow=nrow(m_val))\");",
    "crumbs": [
      "Case Studies",
      "Using R Functions in Stan Models"
    ]
  }
]